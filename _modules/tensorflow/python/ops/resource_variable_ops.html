
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>tensorflow.python.ops.resource_variable_ops &#8212; zfit 0.5.6.dev10+gd2b3a86 documentation</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../../_static/zfit-favicon.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../../../../index.html">
    
      <img src="../../../../_static/zfit-logo-light_400x168.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../whats_new/index.html">Whatâ€™s new?</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../getting_started/index.html">Getting started</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../user_api/index.html">API reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../project/index.html">Project</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../../../../ask_a_question.html">Ask a question</a>
        </li>
        
        
      </ul>


      

      <ul class="navbar-nav">
        
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/zfit/zfit" target="_blank" rel="noopener">
              <span><i class="fab fa-github-square"></i></span>
            </a>
          </li>
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search zfit..." aria-label="Search zfit..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for tensorflow.python.ops.resource_variable_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2016 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Ops to use variables as resources.&quot;&quot;&quot;</span>

<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">weakref</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">attr_value_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">variable_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">_pywrap_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">pywrap_tf_session</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">tape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">auto_control_deps_utils</span> <span class="k">as</span> <span class="n">acd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">cpp_shape_inference_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_logging_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_state_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">state_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">variables</span>
<span class="c1"># go/tf-wildcard-import</span>
<span class="c1"># pylint: disable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_resource_variable_ops</span> <span class="kn">import</span> <span class="o">*</span>
<span class="c1"># pylint: enable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.types</span> <span class="kn">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="kn">import</span> <span class="n">deprecated</span>


<span class="n">acd</span><span class="o">.</span><span class="n">register_read_only_resource_op</span><span class="p">(</span><span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">)</span>
<span class="n">acd</span><span class="o">.</span><span class="n">register_read_only_resource_op</span><span class="p">(</span><span class="s2">&quot;VariableShape&quot;</span><span class="p">)</span>
<span class="n">acd</span><span class="o">.</span><span class="n">register_read_only_resource_op</span><span class="p">(</span><span class="s2">&quot;ResourceGather&quot;</span><span class="p">)</span>
<span class="n">acd</span><span class="o">.</span><span class="n">register_read_only_resource_op</span><span class="p">(</span><span class="s2">&quot;ResourceGatherNd&quot;</span><span class="p">)</span>
<span class="n">acd</span><span class="o">.</span><span class="n">register_read_only_resource_op</span><span class="p">(</span><span class="s2">&quot;_ReadVariablesOp&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_resource_handle_data</span><span class="p">(</span><span class="n">graph_op</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">graph_op</span><span class="p">)</span> <span class="o">==</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># pylint: disable=unidiomatic-typecheck</span>

  <span class="n">handle_data</span> <span class="o">=</span> <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">GetHandleShapeAndType</span><span class="p">(</span>
      <span class="n">graph_op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">graph_op</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">return</span> <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="o">.</span><span class="n">FromString</span><span class="p">(</span>
      <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">handle_data</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get the data handle from the Tensor `handle`.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">handle</span><span class="o">.</span><span class="n">_handle_data</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_resource_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_set_handle_shapes_and_types</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">handle_data</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sets the shape inference result HandleData on tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor` or `EagerTensor`.</span>
<span class="sd">    handle_data: A `CppShapeInferenceResult.HandleData`.</span>
<span class="sd">    graph_mode: A python bool.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="n">handle_data</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">graph_mode</span><span class="p">:</span>
    <span class="k">return</span>

  <span class="c1"># Not an EagerTensor, so a graph tensor.</span>
  <span class="n">shapes</span><span class="p">,</span> <span class="n">types</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">pair</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">pair</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">])</span>
  <span class="n">ranks</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="o">.</span><span class="n">unknown_rank</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[[</span><span class="n">d</span><span class="o">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">dim</span><span class="p">]</span>  <span class="c1"># pylint: disable=g-complex-comprehension</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="o">.</span><span class="n">unknown_rank</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>
  <span class="n">pywrap_tf_session</span><span class="o">.</span><span class="n">TF_GraphSetOutputHandleShapesAndTypes_wrapper</span><span class="p">(</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">shapes</span><span class="p">,</span>
      <span class="n">ranks</span><span class="p">,</span>
      <span class="n">types</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_combine_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concats HandleData from tensors `handle` and `initial_value`.</span>

<span class="sd">  Args:</span>
<span class="sd">    handle: A `Tensor` of dtype `resource`.</span>
<span class="sd">    initial_value: A `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `CppShapeInferenceResult.HandleData`.  If `initial_value` has dtype</span>
<span class="sd">    `variant`, the `HandleData` contains the concatenation of the shape_and_type</span>
<span class="sd">    from both `handle` and `initial_value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If handle, which was returned by VarHandleOp, either has</span>
<span class="sd">      no handle data, or its len(handle_data.shape_and_type) != 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">handle</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">resource</span>

  <span class="n">variable_handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">variable_handle_data</span>

  <span class="n">extra_handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">extra_handle_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">extra_handle_data</span><span class="o">.</span><span class="n">is_set</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">variable_handle_data</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="ow">not</span> <span class="n">variable_handle_data</span><span class="o">.</span><span class="n">is_set</span>
        <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">variable_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Expected VarHandleOp to return a length==1 shape_and_type, &quot;</span>
          <span class="s2">&quot;but saw: &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">variable_handle_data</span><span class="p">,))</span>
    <span class="n">variable_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
        <span class="n">extra_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">variable_handle_data</span>


<span class="k">def</span> <span class="nf">_variable_handle_from_shape_and_dtype</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a variable handle, copying in handle data from `initial_value`.&quot;&quot;&quot;</span>
  <span class="n">container</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_container</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">container</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">container</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
  <span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">handle</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_handle_op</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                                   <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
                                                   <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">container</span><span class="o">=</span><span class="n">container</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">initial_value</span> <span class="o">=</span> <span class="n">handle</span>
  <span class="k">if</span> <span class="n">graph_mode</span><span class="p">:</span>
    <span class="n">full_handle_data</span> <span class="o">=</span> <span class="n">_combine_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">)</span>
    <span class="n">_set_handle_shapes_and_types</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">full_handle_data</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">handle</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># We do not want two distinct ResourceVariable objects for the same</span>
    <span class="c1"># underlying resource in the runtime.</span>
    <span class="c1"># When in eager mode, explicitly ensure so here. When in graph mode, it&#39;s</span>
    <span class="c1"># ensured by always generating different variable names.</span>
    <span class="n">exists</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

    <span class="c1"># We create an assert Op instead of checking right away in order to be</span>
    <span class="c1"># compatible with ASYNC execution mode. Further, since not all devices</span>
    <span class="c1"># support string tensors, we encode the assertion string in the Op name</span>
    <span class="n">gen_logging_ops</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">exists</span><span class="p">),</span> <span class="p">[</span><span class="n">exists</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;EagerVariableNameReuse&quot;</span><span class="p">)</span>

    <span class="n">handle_data</span> <span class="o">=</span> <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="p">()</span>
    <span class="n">handle_data</span><span class="o">.</span><span class="n">is_set</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleShapeAndType</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="o">.</span><span class="n">as_proto</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="o">.</span><span class="n">as_datatype_enum</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
      <span class="n">extra_handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">extra_handle_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">extra_handle_data</span><span class="o">.</span><span class="n">is_set</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">is_set</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
              <span class="s2">&quot;Expected VarHandleOp to return a length==1 shape_and_type, &quot;</span>
              <span class="s2">&quot;but saw: &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle_data</span><span class="p">,))</span>
        <span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
            <span class="n">extra_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span>

    <span class="n">_set_handle_shapes_and_types</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">handle_data</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">handle</span>


<span class="k">def</span> <span class="nf">eager_safe_variable_handle</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                               <span class="n">graph_mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a variable handle with information to do shape inference.</span>

<span class="sd">  The dtype is read from `initial_value` and stored in the returned</span>
<span class="sd">  resource tensor&#39;s handle data.</span>

<span class="sd">  If `initial_value.dtype == tf.variant`, we additionally extract the handle</span>
<span class="sd">  data (if any) from `initial_value` and append it to the `handle_data`.</span>
<span class="sd">  In this case, the returned tensor&#39;s handle data is in the form</span>

<span class="sd">  ```</span>
<span class="sd">  is_set: true</span>
<span class="sd">  shape_and_type {</span>
<span class="sd">    shape {</span>
<span class="sd">      // initial_value.shape</span>
<span class="sd">    }</span>
<span class="sd">    dtype: DT_VARIANT</span>
<span class="sd">  }</span>
<span class="sd">  shape_and_type {</span>
<span class="sd">    // handle_data(initial_value).shape_and_type[0]</span>
<span class="sd">  }</span>
<span class="sd">  shape_and_type {</span>
<span class="sd">    // handle_data(initial_value).shape_and_type[1]</span>
<span class="sd">  }</span>
<span class="sd">  ...</span>
<span class="sd">  ```</span>

<span class="sd">  Ops that read from this tensor, such as `ReadVariableOp` and</span>
<span class="sd">  `AssignVariableOp`, know that `handle_data(handle).shape_and_type[1:]`</span>
<span class="sd">  correspond to the handle data of the variant(s) stored in the Variable.</span>

<span class="sd">  Args:</span>
<span class="sd">    initial_value: A `Tensor`.</span>
<span class="sd">    shape: The shape of the handle data. Can be `TensorShape(None)`</span>
<span class="sd">      (i.e. unknown shape).</span>
<span class="sd">    shared_name: A string.</span>
<span class="sd">    name: A string.</span>
<span class="sd">    graph_mode: A python bool.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The handle, a `Tensor` of type `resource`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="k">return</span> <span class="n">_variable_handle_from_shape_and_dtype</span><span class="p">(</span>
      <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">)</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_handle_graph</span><span class="p">(</span><span class="n">handle</span><span class="p">):</span>
  <span class="c1"># Note: might have an eager tensor but not be executing eagerly when building</span>
  <span class="c1"># functions.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">)</span>
      <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">has_default_graph</span><span class="p">()):</span>
    <span class="k">yield</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">handle</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">yield</span>


<span class="k">class</span> <span class="nc">EagerResourceDeleter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An object which cleans up a resource handle.</span>

<span class="sd">  An alternative to defining a __del__ method on an object. The intended use is</span>
<span class="sd">  that ResourceVariables or other objects with resource handles will maintain a</span>
<span class="sd">  single reference to this object. When the parent object is collected, this</span>
<span class="sd">  object will be too. Even if the parent object is part of a reference cycle,</span>
<span class="sd">  the cycle will be collectable.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">handle_device</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="p">(</span><span class="s2">&quot;Passed handle=</span><span class="si">%s</span><span class="s2"> to EagerResourceDeleter. Was expecting a handle &quot;</span>
           <span class="s2">&quot;Tensor.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle</span><span class="p">,)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">handle</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_device</span> <span class="o">=</span> <span class="n">handle_device</span>
    <span class="c1"># This is held since the __del__ function runs an op, and if the context()</span>
    <span class="c1"># is collected before this object, there will be a segfault when running the</span>
    <span class="c1"># op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Resources follow object-identity when executing eagerly, so it is safe to</span>
    <span class="c1"># delete the resource we have a handle to.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># A packed EagerTensor doesn&#39;t own any resource.</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">is_packed</span><span class="p">:</span>
        <span class="k">return</span>
      <span class="c1"># This resource was created in eager mode. However, this destructor may be</span>
      <span class="c1"># running in graph mode (especially during unit tests). To clean up</span>
      <span class="c1"># successfully, we switch back into eager mode temporarily.</span>
      <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">eager_mode</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle_device</span><span class="p">):</span>
          <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">destroy_resource_op</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">ignore_lookup_error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
      <span class="c1"># Suppress some exceptions, mainly for the case when we&#39;re running on</span>
      <span class="c1"># module deletion. Things that can go wrong include the context module</span>
      <span class="c1"># already being unloaded, self._handle._handle_data no longer being</span>
      <span class="c1"># valid, and so on. Printing warnings in these cases is silly</span>
      <span class="c1"># (exceptions raised from __del__ are printed as warnings to stderr).</span>
      <span class="k">pass</span>  <span class="c1"># &#39;NoneType&#39; object is not callable when the handle has been</span>
      <span class="c1"># partially unloaded.</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">pass</span>  <span class="c1"># &#39;NoneType&#39; object has no attribute &#39;eager_mode&#39; when context has</span>
      <span class="c1"># been unloaded. Will catch other module unloads as well.</span>


<span class="k">def</span> <span class="nf">shape_safe_assign_variable_handle</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper that checks shape compatibility and assigns variable.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="n">handle</span><span class="p">):</span>
    <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
  <span class="n">shape</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">value_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span>
                                                      <span class="n">value_tensor</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
    <span class="c1"># For DT_VARIANT types, the handle&#39;s shape_and_type[1:] stores the</span>
    <span class="c1"># variant&#39;s handle data.  Extract it.</span>
    <span class="n">handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">is_set</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="p">(</span>
              <span class="n">is_set</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">shape_and_type</span><span class="o">=</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>


<span class="k">def</span> <span class="nf">variable_accessed</span><span class="p">(</span><span class="n">variable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Records that `variable` was accessed for the tape and FuncGraph.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="s2">&quot;watch_variable&quot;</span><span class="p">):</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">watch_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">variable</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">variable_accessed</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BaseResourceVariable</span><span class="p">(</span><span class="n">variables</span><span class="o">.</span><span class="n">VariableV1</span><span class="p">,</span> <span class="n">core</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A python variable from an existing handle.&quot;&quot;&quot;</span>

  <span class="c1"># TODO(wangpeng): Deprecate `constraint` when callers no long pass it in.</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">handle</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">unique_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">handle_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">graph_element</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">initializer_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">is_initialized_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">cached_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">save_slice_info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">handle_deleter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="o">**</span><span class="n">unused_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable from a handle.</span>

<span class="sd">    Args:</span>
<span class="sd">      trainable: If `True`, GradientTapes automatically watch uses of this</span>
<span class="sd">        Variable.</span>
<span class="sd">      shape: The variable&#39;s shape.</span>
<span class="sd">      dtype: The variable&#39;s dtype.</span>
<span class="sd">      handle: The variable&#39;s handle</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      distribute_strategy: The distribution strategy this variable was created</span>
<span class="sd">        under.</span>
<span class="sd">      name: The name for this variable.</span>
<span class="sd">      unique_id: Internal. Unique ID for this variable&#39;s handle.</span>
<span class="sd">      handle_name: The name for the variable&#39;s handle.</span>
<span class="sd">      graph_element: Optional, required only in session.run-mode. Pre-created</span>
<span class="sd">        tensor which reads this variable&#39;s value.</span>
<span class="sd">      initial_value: Optional. Variable&#39;s initial value.</span>
<span class="sd">      initializer_op: Operation which assigns the variable&#39;s initial value.</span>
<span class="sd">      is_initialized_op: Pre-created operation to check whether this variable</span>
<span class="sd">        is initialized.</span>
<span class="sd">      cached_value: Pre-created operation to read this variable in a specific</span>
<span class="sd">        device.</span>
<span class="sd">      save_slice_info: Metadata for variable partitioning.</span>
<span class="sd">      handle_deleter: EagerResourceDeleter responsible for cleaning up the</span>
<span class="sd">        handle.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span> <span class="o">=</span> <span class="n">synchronization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="n">save_slice_info</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">initial_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">initializer_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_initialized_op</span> <span class="o">=</span> <span class="n">is_initialized_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">graph_element</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="n">caching_device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="n">cached_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_distribute_strategy</span> <span class="o">=</span> <span class="n">distribute_strategy</span>
    <span class="c1"># Store the graph key so optimizers know how to only retrieve variables from</span>
    <span class="c1"># this graph. Guaranteed to be the same as the eager graph_key.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_graph_key</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">handle</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">graph_element</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span> <span class="o">=</span> <span class="n">unique_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span> <span class="o">=</span> <span class="n">handle_name</span> <span class="o">+</span> <span class="s2">&quot;:0&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="n">constraint</span>
    <span class="c1"># After the handle has been created, set up a way to clean it up when</span>
    <span class="c1"># executing eagerly. We&#39;ll hold the only reference to the deleter, so that</span>
    <span class="c1"># when this object is garbage collected the deleter will be too. This</span>
    <span class="c1"># means ResourceVariables can be part of reference cycles without those</span>
    <span class="c1"># cycles being uncollectable.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">handle_deleter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">handle_deleter</span> <span class="o">=</span> <span class="n">EagerResourceDeleter</span><span class="p">(</span>
            <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">handle_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_deleter</span> <span class="o">=</span> <span class="n">handle_deleter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_shape_as_list</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="c1"># If we cannot read the value for any reason, still produce a __repr__.</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">value_text</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">(),</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">except</span><span class="p">:</span>  <span class="c1"># pylint: disable=bare-except</span>
        <span class="n">value_text</span> <span class="o">=</span> <span class="s2">&quot;&lt;unavailable&gt;&quot;</span>

      <span class="k">return</span> <span class="s2">&quot;&lt;tf.Variable &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">, numpy=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">value_text</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;&lt;tf.Variable &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_assign_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Makes assignments depend on the cached value, if any.</span>

<span class="sd">    This prevents undefined behavior with reads not ordered wrt writes.</span>

<span class="sd">    Yields:</span>
<span class="sd">      None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span><span class="p">]):</span>
        <span class="k">yield</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">yield</span>

  <span class="k">def</span> <span class="nf">__nonzero__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__bool__</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="s2">&quot;__deepcopy__() is only available when eager execution is enabled.&quot;</span><span class="p">)</span>
    <span class="n">copied_variable</span> <span class="o">=</span> <span class="n">ResourceVariable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">(),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_name</span><span class="p">,</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_distribute_strategy</span><span class="p">)</span>
    <span class="n">memo</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">copied_variable</span>
    <span class="k">return</span> <span class="n">copied_variable</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The dtype of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The device this variable is on.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the handle for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shape of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="o">.</span><span class="n">merge_with</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_shape_as_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_shape_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op responsible for initializing this variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Calling create is not supported when eager execution&quot;</span>
                         <span class="s2">&quot; is enabled.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The handle by which this variable can be accessed.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span>

  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A cached operation which reads the value of this variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_as_graph_element</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Conversion function for Graph.as_graph_element().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op responsible for initializing this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initial_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Tensor used as the initial value for the variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;initial_value not supported in EAGER mode.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the constraint function associated with this variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The constraint function that was passed to the variable constructor.</span>
<span class="sd">      Can be `None` if no constraint was passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">synchronization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">aggregation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates and returns the value of this variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Trying to eval in EAGER mode&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;numpy() is only available when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Prefer Dataset.range instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Increments this variable until it reaches `limit`.</span>

<span class="sd">    When that Op is run it tries to increment the variable by `1`. If</span>
<span class="sd">    incrementing the variable would bring it above `limit` then the Op raises</span>
<span class="sd">    the exception `OutOfRangeError`.</span>

<span class="sd">    If no error is raised, the Op outputs the value of the variable before</span>
<span class="sd">    the increment.</span>

<span class="sd">    This is essentially a shortcut for `count_up_to(self, limit)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      limit: value at which incrementing the variable raises an error.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the variable value before the increment. If no</span>
<span class="sd">      other Op modifies this variable, the values produced will all be</span>
<span class="sd">      distinct.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">,</span>
                                              <span class="n">T</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_map_resources</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For implementing `Trackable`.&quot;&quot;&quot;</span>
    <span class="n">new_variable</span> <span class="o">=</span> <span class="n">copy_to_graph_uninitialized</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">obj_map</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="p">:</span> <span class="n">new_variable</span><span class="p">}</span>
    <span class="n">resource_map</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">:</span> <span class="n">new_variable</span><span class="o">.</span><span class="n">handle</span><span class="p">}</span>
    <span class="k">return</span> <span class="n">obj_map</span><span class="p">,</span> <span class="n">resource_map</span>

  <span class="k">def</span> <span class="nf">_read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">read_and_set_handle</span><span class="p">():</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">result</span>

    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_caching_device&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span><span class="p">):</span>
          <span class="n">result</span> <span class="o">=</span> <span class="n">read_and_set_handle</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">read_and_set_handle</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># Note that if a control flow context is active the input of the read op</span>
      <span class="c1"># might not actually be the handle. This line bypasses it.</span>
      <span class="n">tape</span><span class="o">.</span><span class="n">record_operation</span><span class="p">(</span>
          <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">result</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">],</span>
          <span class="n">backward_function</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span>
          <span class="n">forward_function</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">read_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an op which reads the value of this variable.</span>

<span class="sd">    Should be used when there are multiple reads, or when it is desirable to</span>
<span class="sd">    read the value only after some condition is true.</span>

<span class="sd">    Returns:</span>
<span class="sd">     the read operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Read&quot;</span><span class="p">):</span>
      <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>
    <span class="c1"># Return an identity so it can get placed on whatever device the context</span>
    <span class="c1"># specifies instead of the device where the variable is.</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">sparse_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reads the value of this variable sparsely, using `gather`.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Gather&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_gather</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
        <span class="c1"># For DT_VARIANT types, the handle&#39;s shape_and_type[1:] stores the</span>
        <span class="c1"># variant&#39;s handle data.  Extract it.</span>
        <span class="n">handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">is_set</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
          <span class="n">value</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
              <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="p">(</span>
                  <span class="n">is_set</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">shape_and_type</span><span class="o">=</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reads the value of this variable sparsely, using `gather_nd`.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;GatherNd&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
        <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_gather_nd</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a `ResourceVariable` to a `VariableDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If run in EAGER mode.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `VariableDef` protocol buffer, or `None` if the `Variable` is not</span>
<span class="sd">      in the specified name scope.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;to_proto not supported in EAGER mode.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">export_scope</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">export_scope</span><span class="p">):</span>
      <span class="n">var_def</span> <span class="o">=</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">()</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">variable_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># This is inside an if-statement for backwards compatibility, since</span>
        <span class="c1"># self._initial_value might be None for variables constructed from old</span>
        <span class="c1"># protos.</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">initial_value_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">initializer_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                      <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">snapshot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Store the graph_element here</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">snapshot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">is_resource</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">synchronization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">synchronization</span><span class="o">.</span><span class="n">value</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">aggregation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="o">.</span><span class="n">value</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span><span class="p">:</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">var_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_proto</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;from_proto not supported in EAGER mode.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ResourceVariable</span><span class="p">(</span>
        <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>

  <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">100</span>

  <span class="k">def</span> <span class="nf">is_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks whether a resource variable has been initialized.</span>

<span class="sd">    Outputs boolean scalar indicating whether the tensor has been initialized.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` of type `bool`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">assign_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts a value from this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to subtract from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name to use for the operation.</span>
<span class="sd">      read_value: A `bool`. Whether to read and return the new value of the</span>
<span class="sd">          variable or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      If `read_value` is `True`, this method will return the new value of the</span>
<span class="sd">      variable after the assignment has completed. Otherwise, when in graph mode</span>
<span class="sd">      it will return the `Operation` that does the assignment, and when in eager</span>
<span class="sd">      mode it will return `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO(apassos): this here and below is not atomic. Consider making it</span>
    <span class="c1"># atomic if there&#39;s a way to do so without a performance cost for those who</span>
    <span class="c1"># don&#39;t need it.</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_dependencies</span><span class="p">():</span>
      <span class="n">assign_sub_op</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_sub_variable_op</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">assign_sub_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assign_sub_op</span>

  <span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a value to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to add to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name to use for the operation.</span>
<span class="sd">      read_value: A `bool`. Whether to read and return the new value of the</span>
<span class="sd">          variable or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      If `read_value` is `True`, this method will return the new value of the</span>
<span class="sd">      variable after the assignment has completed. Otherwise, when in graph mode</span>
<span class="sd">      it will return the `Operation` that does the assignment, and when in eager</span>
<span class="sd">      mode it will return `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_dependencies</span><span class="p">():</span>
      <span class="n">assign_add_op</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_add_variable_op</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">assign_add_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assign_add_op</span>

  <span class="k">def</span> <span class="nf">_lazy_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_UnreadVariable</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
        <span class="n">in_graph_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">,</span>
        <span class="n">deleter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle_deleter</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parent_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span> <span class="n">unique_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns a new value to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: A `Tensor`. The new value for this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the assignment.</span>
<span class="sd">      name: The name to use for the assignment.</span>
<span class="sd">      read_value: A `bool`. Whether to read and return the new value of the</span>
<span class="sd">          variable or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      If `read_value` is `True`, this method will return the new value of the</span>
<span class="sd">      variable after the assignment has completed. Otherwise, when in graph mode</span>
<span class="sd">      it will return the `Operation` that does the assignment, and when in eager</span>
<span class="sd">      mode it will return `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note: not depending on the cached value here since this can be used to</span>
    <span class="c1"># initialize the variable.</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
      <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">value_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="n">assign_op</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">assign_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assign_op</span>

  <span class="k">def</span> <span class="nf">__reduce__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># The implementation mirrors that of __deepcopy__.</span>
    <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">ResourceVariable</span><span class="p">,</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_name</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_distribute_strategy</span><span class="p">),</span> <span class="p">()</span>

  <span class="k">def</span> <span class="nf">scatter_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts `tf.IndexedSlices` from this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be added to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the max of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of max</span>
<span class="sd">        with this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_max</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the min of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of min</span>
<span class="sd">        with this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_min</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multiply this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to multiply this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_mul</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Divide this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to divide this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_div</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">batch_scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable batch-wise.</span>

<span class="sd">    Analogous to `batch_gather`. This assumes that this variable and the</span>
<span class="sd">    sparse_delta IndexedSlices have a series of leading dimensions that are the</span>
<span class="sd">    same for all of them, and the updates are performed on the last dimension of</span>
<span class="sd">    indices. In other words, the dimensions should be the following:</span>

<span class="sd">    `num_prefix_dims = sparse_delta.indices.ndims - 1`</span>
<span class="sd">    `batch_dim = num_prefix_dims + 1`</span>
<span class="sd">    `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[</span>
<span class="sd">         batch_dim:]`</span>

<span class="sd">    where</span>

<span class="sd">    `sparse_delta.updates.shape[:num_prefix_dims]`</span>
<span class="sd">    `== sparse_delta.indices.shape[:num_prefix_dims]`</span>
<span class="sd">    `== var.shape[:num_prefix_dims]`</span>

<span class="sd">    And the operation performed can be expressed as:</span>

<span class="sd">    `var[i_1, ..., i_n,</span>
<span class="sd">         sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[</span>
<span class="sd">            i_1, ..., i_n, j]`</span>

<span class="sd">    When sparse_delta.indices is a 1D tensor, this operation is equivalent to</span>
<span class="sd">    `scatter_update`.</span>

<span class="sd">    To avoid this operation one can looping over the first `ndims` of the</span>
<span class="sd">    variable and using `scatter_update` on the subtensors that result of slicing</span>
<span class="sd">    the first dimension. This is a valid option for `ndims = 1`, but less</span>
<span class="sd">    efficient than this implementation.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">state_ops</span><span class="o">.</span><span class="n">batch_scatter_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse subtraction to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = ref.scatter_nd_sub(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, -9, 3, -6, -6, 6, 7, -4]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse addition to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        add = ref.scatter_nd_add(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(add)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, 13, 3, 14, 14, 6, 7, 20]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse assignment to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = ref.scatter_nd_update(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, 11, 3, 10, 9, 6, 7, 12]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the max of `tf.IndexedSlices` and itself.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span>
        <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_max</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span>
            <span class="n">indices</span><span class="p">,</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the min of `tf.IndexedSlices` and itself.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The updated variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span>
        <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_min</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span>
            <span class="n">indices</span><span class="p">,</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_strided_slice_assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">begin_mask</span><span class="p">,</span>
                            <span class="n">end_mask</span><span class="p">,</span> <span class="n">ellipsis_mask</span><span class="p">,</span> <span class="n">new_axis_mask</span><span class="p">,</span>
                            <span class="n">shrink_axis_mask</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_dependencies</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span>
          <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">resource_strided_slice_assign</span><span class="p">(</span>
              <span class="n">ref</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span>
              <span class="n">begin</span><span class="o">=</span><span class="n">begin</span><span class="p">,</span>
              <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">,</span>
              <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
              <span class="n">value</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
              <span class="n">begin_mask</span><span class="o">=</span><span class="n">begin_mask</span><span class="p">,</span>
              <span class="n">end_mask</span><span class="o">=</span><span class="n">end_mask</span><span class="p">,</span>
              <span class="n">ellipsis_mask</span><span class="o">=</span><span class="n">ellipsis_mask</span><span class="p">,</span>
              <span class="n">new_axis_mask</span><span class="o">=</span><span class="n">new_axis_mask</span><span class="p">,</span>
              <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="n">shrink_axis_mask</span><span class="p">))</span>

  <span class="k">def</span> <span class="fm">__complex__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">complex</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__long__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">long</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="fm">__float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">_dense_var_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">name</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Incompatible type conversion requested to type </span><span class="si">{!r}</span><span class="s2"> for variable &quot;</span>
          <span class="s2">&quot;of type </span><span class="si">{!r}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">as_ref</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable += value not supported. Use &quot;</span>
                       <span class="s2">&quot;variable.assign_add(value) to modify the variable &quot;</span>
                       <span class="s2">&quot;value and variable = variable + value to get a new &quot;</span>
                       <span class="s2">&quot;Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable -= value not supported. Use &quot;</span>
                       <span class="s2">&quot;variable.assign_sub(value) to modify the variable &quot;</span>
                       <span class="s2">&quot;value and variable = variable - value to get a new &quot;</span>
                       <span class="s2">&quot;Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable *= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var * value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var * value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__idiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable /= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var / value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var / value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable /= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var / value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var / value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__irealdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable /= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var / value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var / value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable **= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var ** value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var ** value` to get a new Tensor object.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResourceVariable</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variable based on resource handles.</span>

<span class="sd">  See the [Variables How To](https://tensorflow.org/guide/variables)</span>
<span class="sd">  for a high level overview.</span>

<span class="sd">  A `ResourceVariable` allows you to maintain state across subsequent calls to</span>
<span class="sd">  session.run.</span>

<span class="sd">  The `ResourceVariable` constructor requires an initial value for the variable,</span>
<span class="sd">  which can be a `Tensor` of any type and shape. The initial value defines the</span>
<span class="sd">  type and shape of the variable. After construction, the type and shape of</span>
<span class="sd">  the variable are fixed. The value can be changed using one of the assign</span>
<span class="sd">  methods.</span>

<span class="sd">  Just like any `Tensor`, variables created with</span>
<span class="sd">  `tf.Variable(use_resource=True)` can be used as inputs for other Ops in the</span>
<span class="sd">  graph. Additionally, all the operators overloaded for the `Tensor` class are</span>
<span class="sd">  carried over to variables, so you can also add nodes to the graph by just</span>
<span class="sd">  doing arithmetic on variables.</span>

<span class="sd">  Unlike ref-based variable, a ResourceVariable has well-defined semantics. Each</span>
<span class="sd">  usage of a ResourceVariable in a TensorFlow graph adds a read_value operation</span>
<span class="sd">  to the graph. The Tensors returned by a read_value operation are guaranteed to</span>
<span class="sd">  see all modifications to the value of the variable which happen in any</span>
<span class="sd">  operation on which the read_value depends on (either directly, indirectly, or</span>
<span class="sd">  via a control dependency) and guaranteed to not see any modification to the</span>
<span class="sd">  value of the variable from operations that depend on the read_value operation.</span>
<span class="sd">  Updates from operations that have no dependency relationship to the read_value</span>
<span class="sd">  operation might or might not be visible to read_value.</span>

<span class="sd">  For example, if there is more than one assignment to a ResourceVariable in</span>
<span class="sd">  a single session.run call there is a well-defined value for each operation</span>
<span class="sd">  which uses the variable&#39;s value if the assignments and the read are connected</span>
<span class="sd">  by edges in the graph. Consider the following example, in which two writes</span>
<span class="sd">  can cause tf.Variable and tf.ResourceVariable to behave differently:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.Variable(1.0, use_resource=True)</span>
<span class="sd">  a.initializer.run()</span>

<span class="sd">  assign = a.assign(2.0)</span>
<span class="sd">  with tf.control_dependencies([assign]):</span>
<span class="sd">    b = a.read_value()</span>
<span class="sd">  with tf.control_dependencies([b]):</span>
<span class="sd">    other_assign = a.assign(3.0)</span>
<span class="sd">  with tf.control_dependencies([other_assign]):</span>
<span class="sd">    # Will print 2.0 because the value was read before other_assign ran. If</span>
<span class="sd">    # `a` was a tf.Variable instead, 2.0 or 3.0 could be printed.</span>
<span class="sd">    tf.compat.v1.Print(b, [b]).eval()</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
               <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># pylint: disable=unused-argument</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called.</span>
<span class="sd">        (Note that initializer functions from init_ops.py must first be bound</span>
<span class="sd">        to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, the default, also adds the variable to the graph</span>
<span class="sd">        collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as</span>
<span class="sd">        the default list of variables to use by the `Optimizer` classes.</span>
<span class="sd">        Defaults to `True`, unless `synchronization` is set to `ON_READ`, in</span>
<span class="sd">        which case it defaults to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      validate_shape: Ignored. Provided for compatibility with tf.Variable.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If None, either the datatype will be kept (if initial_value is</span>
<span class="sd">        a Tensor) or float32 will be used (if it is a Python object convertible</span>
<span class="sd">        to a Tensor).</span>
<span class="sd">      variable_def: `VariableDef` protocol buffer. If not None, recreates the</span>
<span class="sd">        `ResourceVariable` object with its contents. `variable_def` and other</span>
<span class="sd">        arguments (except for import_scope) are mutually exclusive.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add to the</span>
<span class="sd">        ResourceVariable. Only used when `variable_def` is provided.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      distribute_strategy: The tf.distribute.Strategy this variable is being</span>
<span class="sd">        created inside of.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>

<span class="sd">    @compatibility(eager)</span>
<span class="sd">    When Eager Execution is enabled, the default for the `collections` argument</span>
<span class="sd">    is `None`, which signifies that this `Variable` will not be added to any</span>
<span class="sd">    collections.</span>
<span class="sd">    @end_compatibility</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;variable_def and initial_value are mutually &quot;</span>
                         <span class="s2">&quot;exclusive.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Creating ResourceVariable from variable_def is &quot;</span>
                         <span class="s2">&quot;not supported when eager execution is enabled.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_proto</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_args</span><span class="p">(</span>
          <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
          <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_init_from_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called.</span>
<span class="sd">        (Note that initializer functions from init_ops.py must first be bound</span>
<span class="sd">         to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, the default, also adds the variable to the graph</span>
<span class="sd">        collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as</span>
<span class="sd">        the default list of variables to use by the `Optimizer` classes.</span>
<span class="sd">        Defaults to `True`, unless `synchronization` is set to `ON_READ`, in</span>
<span class="sd">        which case it defaults to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If None, either the datatype will be kept (if initial_value is</span>
<span class="sd">       a Tensor) or float32 will be used (if it is a Python object convertible</span>
<span class="sd">       to a Tensor).</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      distribute_strategy: DistributionStrategy under which this variable</span>
<span class="sd">        was created.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>

<span class="sd">    @compatibility(eager)</span>
<span class="sd">    When Eager Execution is enabled, variables are never added to collections.</span>
<span class="sd">    It is not implicitly added to the `GLOBAL_VARIABLES` or</span>
<span class="sd">    `TRAINABLE_VARIABLES` collections, and the `collections` argument is</span>
<span class="sd">    ignored.</span>
<span class="sd">    @end_compatibility</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;initial_value must be specified.&quot;</span><span class="p">)</span>
    <span class="n">init_from_fn</span> <span class="o">=</span> <span class="n">callable</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor-typed variable initializers must either be &quot;</span>
                       <span class="s2">&quot;wrapped in an init_scope or callable &quot;</span>
                       <span class="s2">&quot;(e.g., `tf.Variable(lambda : &quot;</span>
                       <span class="s2">&quot;tf.truncated_normal([10, 40]))`) when building &quot;</span>
                       <span class="s2">&quot;functions. Please file a feature request if this &quot;</span>
                       <span class="s2">&quot;restriction inconveniences you.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;collections argument to Variable constructor must be a list, tuple, &quot;</span>
          <span class="s2">&quot;or set. Got </span><span class="si">%s</span><span class="s2"> of type </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">collections</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">constraint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `constraint` argument must be a callable.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">trackable</span><span class="o">.</span><span class="n">CheckpointInitialValue</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_initialize_trackable</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_update_uid</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">checkpoint_position</span><span class="o">.</span><span class="n">restore_uid</span>
      <span class="n">initial_value</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">wrapped_value</span>

    <span class="k">if</span> <span class="n">trainable</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">collections</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">collections</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span>
          <span class="n">name</span><span class="p">,</span>
          <span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="p">[</span><span class="n">initial_value</span><span class="p">],</span>
          <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">handle_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">handle_name</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="n">shared_name</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># When in eager mode use a uid for the shared_name, to prevent</span>
          <span class="c1"># accidental sharing.</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle_name</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">uid</span><span class="p">())</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shared_name</span><span class="p">()</span>
        <span class="c1"># Use attr_scope and device(None) to simulate the behavior of</span>
        <span class="c1"># colocate_with when the variable we want to colocate with doesn&#39;t</span>
        <span class="c1"># yet exist.</span>
        <span class="n">device_context_manager</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="k">else</span> <span class="n">ops</span><span class="o">.</span><span class="n">NullContextmanager</span><span class="p">)</span>
        <span class="n">attr</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>
            <span class="nb">list</span><span class="o">=</span><span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span>
                <span class="n">s</span><span class="o">=</span><span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="s2">&quot;loc:@</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">handle_name</span><span class="p">)]))</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_attr_scope</span><span class="p">({</span><span class="s2">&quot;_class&quot;</span><span class="p">:</span> <span class="n">attr</span><span class="p">}):</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Initializer&quot;</span><span class="p">),</span> <span class="n">device_context_manager</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
            <span class="n">initial_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                <span class="n">initial_value</span><span class="p">()</span> <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="n">initial_value</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
              <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                  <span class="s2">&quot;The initial value&#39;s shape (</span><span class="si">%s</span><span class="s2">) is not compatible with &quot;</span>
                  <span class="s2">&quot;the explicitly supplied `shape` argument (</span><span class="si">%s</span><span class="s2">).&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span>
          <span class="n">handle</span> <span class="o">=</span> <span class="n">eager_safe_variable_handle</span><span class="p">(</span>
              <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
              <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
              <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
              <span class="n">graph_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">)</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="ow">and</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
            <span class="n">initial_value</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s2">&quot;Initializer for variable </span><span class="si">%s</span><span class="s2"> is from inside a control-flow &quot;</span>
              <span class="s2">&quot;construct, such as a loop or conditional. When creating a &quot;</span>
              <span class="s2">&quot;variable inside a loop or conditional, use a lambda as the &quot;</span>
              <span class="s2">&quot;initializer.&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;IsInitialized&quot;</span><span class="p">):</span>
            <span class="n">is_initialized_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="n">handle</span><span class="p">))</span>
          <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># pylint: disable=g-backslash-continuation</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Assign&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">n</span><span class="p">,</span> \
                 <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> \
                 <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
              <span class="c1"># pylint: disable=protected-access</span>
              <span class="n">initializer_op</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
                      <span class="n">handle</span><span class="p">,</span>
                      <span class="n">variables</span><span class="o">.</span><span class="n">_try_guard_against_uninitialized_dependencies</span><span class="p">(</span>
                          <span class="n">name</span><span class="p">,</span>
                          <span class="n">initial_value</span><span class="p">),</span>
                      <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">))</span>
              <span class="c1"># pylint: enable=protected-access</span>
            <span class="c1"># pylint: enable=g-backslash-continuation</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Read&quot;</span><span class="p">):</span>
            <span class="c1"># Manually assign reads to the handle&#39;s device to avoid log</span>
            <span class="c1"># messages.</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
              <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
              <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">graph_element</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">if</span> <span class="n">caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
              <span class="c1"># Variables may be created in a tf.device() or ops.colocate_with()</span>
              <span class="c1"># context. At the same time, users would expect caching device to</span>
              <span class="c1"># be independent of this context, and/or would not expect the</span>
              <span class="c1"># current device context to be merged with the caching device</span>
              <span class="c1"># spec.  Therefore we reset the colocation stack before creating</span>
              <span class="c1"># the cached value. Note that resetting the colocation stack will</span>
              <span class="c1"># also reset the device stack.</span>
              <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">caching_device</span><span class="p">):</span>
                  <span class="n">cached_value</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">cached_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">)</span>
          <span class="n">is_initialized_op</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="n">initializer_op</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="k">if</span> <span class="n">caching_device</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">caching_device</span><span class="p">):</span>
              <span class="n">cached_value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span>
                  <span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
              <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">cached_value</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">cached_value</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="c1"># Store the variable object so that the original variable can be</span>
          <span class="c1"># accessed to generate functions that are compatible with SavedModel.</span>
          <span class="n">cached_value</span><span class="o">.</span><span class="n">_cached_variable</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="c1"># Eager variables are only added to collections if they are part of an</span>
          <span class="c1"># eager variable store (otherwise in an interactive session they would</span>
          <span class="c1"># hog memory and cause OOM). This is done in ops/variable_scope.py.</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_STEP</span> <span class="ow">in</span> <span class="n">collections</span><span class="p">:</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_STEP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
      <span class="n">initial_value</span> <span class="o">=</span> <span class="n">initial_value</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="k">else</span> <span class="kc">None</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">ResourceVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span> <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">unique_id</span><span class="o">=</span><span class="n">unique_id</span><span class="p">,</span> <span class="n">handle_name</span><span class="o">=</span><span class="n">handle_name</span><span class="p">,</span>
          <span class="n">graph_element</span><span class="o">=</span><span class="n">graph_element</span><span class="p">,</span> <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
          <span class="n">initializer_op</span><span class="o">=</span><span class="n">initializer_op</span><span class="p">,</span> <span class="n">is_initialized_op</span><span class="o">=</span><span class="n">is_initialized_op</span><span class="p">,</span>
          <span class="n">cached_value</span><span class="o">=</span><span class="n">cached_value</span><span class="p">,</span> <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_init_from_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes from `VariableDef` proto.&quot;&quot;&quot;</span>
    <span class="c1"># Note that init_from_proto is currently not supported in Eager mode.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">is_resource</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to restore Variable as ResourceVariable.&quot;</span><span class="p">)</span>

    <span class="c1"># Create from variable_def.</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">variable_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">initializer_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="c1"># Check whether initial_value_name exists for backwards compatibility.</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="s2">&quot;initial_value_name&quot;</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">variable_def</span><span class="o">.</span><span class="n">initial_value_name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">variable_def</span><span class="o">.</span><span class="n">initial_value_name</span><span class="p">,</span>
                                 <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">synchronization</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">variable_name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span> <span class="o">=</span> <span class="n">synchronization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">snapshot_name</span><span class="p">:</span>
      <span class="n">snapshot</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
              <span class="n">variable_def</span><span class="o">.</span><span class="n">snapshot_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="n">snapshot</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">while</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">snapshot</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="c1"># Legacy case for protos without the snapshot name; assume it&#39;s the</span>
      <span class="c1"># following.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/Read/ReadVariableOp:0&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;save_slice_info_def&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">SaveSliceInfo</span><span class="p">(</span>
          <span class="n">save_slice_info_def</span><span class="o">=</span><span class="n">variable_def</span><span class="o">.</span><span class="n">save_slice_info_def</span><span class="p">,</span>
          <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">UninitializedVariable</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A variable with no initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">extra_handle_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="o">**</span><span class="n">unused_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates the variable handle.</span>

<span class="sd">    Args:</span>
<span class="sd">      trainable: If `True`, GradientTapes automatically watch uses of this</span>
<span class="sd">        Variable.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      shape: The variable&#39;s shape.</span>
<span class="sd">      dtype: The variable&#39;s dtype.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      extra_handle_data: Optional, another resource handle or Tensor with handle</span>
<span class="sd">        data to merge with `shape` and `dtype`.</span>
<span class="sd">      distribute_strategy: The tf.distribute.Strategy this variable is being</span>
<span class="sd">        created inside of.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">skip_on_eager</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">handle_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">handle_name</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="n">shared_name</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle_name</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">uid</span><span class="p">())</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shared_name</span><span class="p">(</span><span class="n">unique_id</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">_variable_handle_from_shape_and_dtype</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">graph_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">,</span>
            <span class="n">initial_value</span><span class="o">=</span><span class="n">extra_handle_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Read&quot;</span><span class="p">):</span>
            <span class="c1"># Manually assign reads to the handle&#39;s device to avoid log</span>
            <span class="c1"># messages.</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
              <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
              <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">graph_element</span> <span class="o">=</span> <span class="n">value</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
          <span class="c1"># Do *not* add to TRAINABLE_VARIABLES here, even if self._trainable,</span>
          <span class="c1"># because retraining or frozen use of imported SavedModels is</span>
          <span class="c1"># controlled at higher levels of model building.</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">UninitializedVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">unique_id</span><span class="o">=</span><span class="n">unique_id</span><span class="p">,</span> <span class="n">handle_name</span><span class="o">=</span><span class="n">handle_name</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span> <span class="n">graph_element</span><span class="o">=</span><span class="n">graph_element</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>


<span class="n">_pywrap_utils</span><span class="o">.</span><span class="n">RegisterType</span><span class="p">(</span><span class="s2">&quot;ResourceVariable&quot;</span><span class="p">,</span> <span class="n">ResourceVariable</span><span class="p">)</span>
<span class="n">math_ops</span><span class="o">.</span><span class="n">_resource_variable_type</span> <span class="o">=</span> <span class="n">ResourceVariable</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">_dense_var_to_tensor</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">var</span><span class="o">.</span><span class="n">_dense_var_to_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="c1"># Register a conversion function which reads the value of the variable,</span>
<span class="c1"># allowing instances of the class to be used as tensors.</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">,</span>
                                        <span class="n">_dense_var_to_tensor</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_UnreadVariable</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents a future for a read of a variable.</span>

<span class="sd">  Pretends to be the tensor if anyone looks.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">in_graph_mode</span><span class="p">,</span> <span class="n">deleter</span><span class="p">,</span>
               <span class="n">parent_op</span><span class="p">,</span> <span class="n">unique_id</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">handle_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">handle_name</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">name</span>
    <span class="c1"># Only create a graph_element if we&#39;re in session.run-land as only</span>
    <span class="c1"># session.run requires a preexisting tensor to evaluate. Otherwise we can</span>
    <span class="c1"># avoid accidentally reading the variable.</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">inside_function</span><span class="p">():</span>
      <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">parent_op</span><span class="p">]):</span>
        <span class="n">graph_element</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span>
            <span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">graph_element</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">handle_name</span><span class="o">=</span><span class="n">handle_name</span><span class="p">,</span>
        <span class="n">unique_id</span><span class="o">=</span><span class="n">unique_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle_deleter</span><span class="o">=</span><span class="n">deleter</span><span class="p">,</span>
        <span class="n">graph_element</span><span class="o">=</span><span class="n">graph_element</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span> <span class="o">=</span> <span class="n">parent_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;UnreadVariable&quot;</span>

  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">read_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">assign_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                                                     <span class="n">read_value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                                                     <span class="n">read_value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                                                 <span class="n">read_value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_sub</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_max</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_min</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_mul</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_div</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_update</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span>
                                                         <span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">batch_scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">batch_scatter_update</span><span class="p">(</span>
          <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_nd_sub</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_nd_add</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_nd_update</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span>
                                                            <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_nd_max</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">scatter_nd_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_nd_min</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_ReadGrad</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for read op.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span>
      <span class="n">handle</span><span class="p">,</span> <span class="s2">&quot;_handle_data&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">handle</span><span class="o">.</span><span class="n">_handle_data</span><span class="o">.</span><span class="n">is_set</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>
  <span class="n">shape_proto</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">shape_proto</span><span class="o">.</span><span class="n">unknown_rank</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">shape_proto</span><span class="o">.</span><span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">shape_proto</span><span class="o">.</span><span class="n">dim</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;ResourceGather&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_GatherGrad</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for gather op.&quot;&quot;&quot;</span>
  <span class="c1"># Build appropriately shaped IndexedSlices</span>
  <span class="n">handle</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">params_shape</span> <span class="o">=</span> <span class="n">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
  <span class="n">size</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">values_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">params_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">values_shape</span><span class="p">)</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">params_shape</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_to_proto_fn</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts Variable and ResourceVariable to VariableDef for collections.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_from_proto_fn</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates Variable or ResourceVariable from VariableDef as needed.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">is_resource</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">ResourceVariable</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>


<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">MOVING_AVERAGE_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">MODEL_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_STEP</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">METRIC_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_resource_variable</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;&quot;Returns True if `var` is to be considered a ResourceVariable.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">BaseResourceVariable</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span>
      <span class="n">var</span><span class="p">,</span> <span class="s2">&quot;_should_act_as_resource_variable&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">copy_to_graph_uninitialized</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Copies an existing variable to a new graph, with no initializer.&quot;&quot;&quot;</span>
  <span class="c1"># Like ResourceVariable.__deepcopy__, but does not set an initializer on the</span>
  <span class="c1"># new variable.</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">new_variable</span> <span class="o">=</span> <span class="n">UninitializedVariable</span><span class="p">(</span>
      <span class="n">trainable</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">_constraint</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">_shared_name</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">synchronization</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
      <span class="n">extra_handle_data</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">handle</span><span class="p">)</span>
  <span class="n">new_variable</span><span class="o">.</span><span class="n">_maybe_initialize_trackable</span><span class="p">()</span>
  <span class="c1"># pylint: enable=protected-access</span>
  <span class="k">return</span> <span class="n">new_variable</span>

<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;Assert&quot;</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;VarIsInitializedOp&quot;</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;VariableShape&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">VariableSpec</span><span class="p">(</span><span class="n">tensor_spec</span><span class="o">.</span><span class="n">DenseSpec</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Describes a tf.Variable.&quot;&quot;&quot;</span>

  <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">value_type</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="n">BaseResourceVariable</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_to_components</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">_from_components</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">components</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

  <span class="k">def</span> <span class="nf">_from_compatible_tensor_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_list</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="n">_pywrap_utils</span><span class="o">.</span><span class="n">RegisterType</span><span class="p">(</span><span class="s2">&quot;VariableSpec&quot;</span><span class="p">,</span> <span class="n">VariableSpec</span><span class="p">)</span>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright Copyright 2018, zfit.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.3.0.<br/>
    </p>
  </div>
</footer>
  </body>
</html>