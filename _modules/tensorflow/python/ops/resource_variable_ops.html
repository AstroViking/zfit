<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
    <title>tensorflow.python.ops.resource_variable_ops &#8212; zfit 0.1.dev567+g48c78ee documentation</title>
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-2.3.2/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-fixed-top">
    <div class="navbar-inner">
      <div class="container">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

        <a class="brand" href="../../../../index.html">
          zfit</a>
        <span class="navbar-text pull-left"><b>0.1.dev567</b></span>

        <div class="nav-collapse">
          <ul class="nav">
            <li class="divider-vertical"></li>
            
                <li><a href="../../../../getting_started.html">Getting started</a></li>
                <li><a href="../../../../introduction.html">Intro</a></li>
                <li><a href="../../../../project.html">Project</a></li>
                <li><a href="../../../../API.html">API</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Overview <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting started with zfit</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../getting_started.html#what-did-just-happen">What did just happen?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../introduction.html">zfit introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/space.html">Space, Observable and Range</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/space.html#definitions">Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/space.html#limits">Limits</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/space.html#defining-limits">Defining limits</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/parameter.html">Parameter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/parameter.html#independent-parameter">Independent Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/parameter.html#dependent-parameter">Dependent Parameter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/model.html">Building a model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#predefined-pdfs-and-basic-properties">Predefined PDFs and basic properties</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#composite-pdf">Composite PDF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#extended-pdf">Extended PDF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/model.html#custom-pdf">Custom PDF</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/model.html#sampling-from-a-model">Sampling from a Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/model.html#tensor-sampling">Tensor sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../intro/model.html#playing-with-toys-multiple-samplings">Playing with toys: Multiple samplings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/data.html">Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/data.html#import-dataset-from-a-root-file">Import dataset from a ROOT file</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/data.html#import-dataset-from-a-pandas-dataframe-or-numpy-ndarray">Import dataset from a pandas DataFrame or Numpy ndarray</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/loss.html">Loss</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/loss.html#adding-constraints">Adding constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/loss.html#simultaneous-fits">Simultaneous fits</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../intro/minimize.html">Minimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../intro/minimize.html#baseline-minimizers">Baseline minimizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../project.html">zfit Project</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/installation.html">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/installation.html#stable-release">Stable release</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/installation.html#from-sources">From sources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/contributing.html">Contributing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/upgrade_guide.html">Upgrade guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/upgrade_guide.html#upgrade-from-zfit-0-3-x-to-0-4-0">Upgrade from zfit 0.3.x to 0.4.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/upgrade_guide.html#dependents">Dependents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/changelog.html">Changelog</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#develop">Develop</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#major-features-and-improvements">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#behavioral-changes">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#bug-fixes-and-small-changes">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#requirement-changes">Requirement changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#thanks">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id3">0.3.7 (6.12.19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id4">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id5">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id6">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id7">Requirement changes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id8">0.3.6 (12.10.19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id9">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id10">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id11">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id12">Requirement changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id13">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id14">0.3.4 (30-07-19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id15">Major Features and Improvements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id16">Behavioral changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id17">Bug fixes and small changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#id18">Thanks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id19">0.3.3 (15-05-19)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id20">0.3.2 (01-05-19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#breaking-changes">Breaking changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#bugfixes">Bugfixes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../project/changelog.html#improvements">Improvements</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id21">0.3.1 (30-04-19)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id22">0.3.0 (2019-03-20)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/changelog.html#id23">0.0.1 (2018-03-22)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../project/authors.html">Credits</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/authors.html#development-lead">Development Lead</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/authors.html#authors">Authors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../project/authors.html#contributors">Contributors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../API.html">zfit API documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.html">zfit package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/zfit.html#subpackages">Subpackages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.core.html">core</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.core.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.basefunc.html">basefunc</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.basemodel.html">basemodel</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.baseobject.html">baseobject</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.basepdf.html">basepdf</a><ul>
<li class="toctree-l7"><a class="reference internal" href="../../../../api/zfit.core.basepdf.html#defining-your-own-pdf">Defining your own pdf</a></li>
</ul>
</li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.constraint.html">constraint</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.data.html">data</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.dependents.html">dependents</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.dimension.html">dimension</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.integration.html">integration</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.interfaces.html">interfaces</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.limits.html">limits</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.loss.html">loss</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.operations.html">operations</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.parameter.html">parameter</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.sample.html">sample</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.core.testing.html">testing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.minimizers.html">minimizers</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.minimizers.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.base_tf.html">base_tf</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.baseminimizer.html">baseminimizer</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.fitresult.html">fitresult</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.interface.html">interface</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.minimizer_minuit.html">minimizer_minuit</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.minimizer_tfp.html">minimizer_tfp</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.minimizers_scipy.html">minimizers_scipy</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.optimizers_tf.html">optimizers_tf</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.minimizers.tf_external_optimizer.html">tf_external_optimizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.models.html">models</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.models.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.basefunctor.html">basefunctor</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.basic.html">basic</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.dist_tfp.html">dist_tfp</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.functions.html">functions</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.functor.html">functor</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.physics.html">physics</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.polynomials.html">polynomials</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.models.special.html">special</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.util.html">util</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.util.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.cache.html">cache</a><ul>
<li class="toctree-l7"><a class="reference internal" href="../../../../api/zfit.util.cache.html#basic-principle">Basic principle</a></li>
</ul>
</li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.checks.html">checks</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.container.html">container</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.diverse.html">diverse</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.exception.html">exception</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.execution.html">execution</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.graph.html">graph</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.logging.html">logging</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.temporary.html">temporary</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.util.ztyping.html">ztyping</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.html">z</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/zfit.z.html#submodules">Submodules</a><ul>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.math.html">math</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.random.html">random</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.tools.html">tools</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.wrapping_tf.html">wrapping_tf</a></li>
<li class="toctree-l6"><a class="reference internal" href="../../../../api/zfit.z.zextension.html">zextension</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/zfit.html#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.constraint.html">constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.data.html">data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.func.html">func</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.loss.html">loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.minimize.html">minimize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.param.html">param</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.pdf.html">pdf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.sample.html">sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.settings.html">settings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.pdf.html">pdf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.func.html">func</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.data.html">data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.loss.html">loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.constraint.html">constraint</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.minimize.html">minimize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.minimizers.fitresult.html">fitresult</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.settings.html">settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/zfit.z.html">z</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/zfit.z.html#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.math.html">math</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.random.html">random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.tools.html">tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.wrapping_tf.html">wrapping_tf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/zfit.z.zextension.html">zextension</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
      </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body span12 content" role="main">
      
  <h1>Source code for tensorflow.python.ops.resource_variable_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2016 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Ops to use variables as resources.&quot;&quot;&quot;</span>

<span class="c1"># pylint: disable=g-bad-name</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">functools</span>

<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">attr_value_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.core.framework</span> <span class="kn">import</span> <span class="n">variable_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">pywrap_tensorflow</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">tape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">cpp_shape_inference_pb2</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_logging_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_resource_variable_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_state_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">state_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">variables</span>
<span class="c1"># go/tf-wildcard-import</span>
<span class="c1"># pylint: disable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops.gen_resource_variable_ops</span> <span class="kn">import</span> <span class="o">*</span>
<span class="c1"># pylint: enable=wildcard-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="kn">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">compat</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util.deprecation</span> <span class="kn">import</span> <span class="n">deprecated_args</span>


<span class="k">def</span> <span class="nf">get_resource_handle_data</span><span class="p">(</span><span class="n">graph_op</span><span class="p">):</span>
  <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">graph_op</span><span class="p">)</span> <span class="o">==</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># pylint: disable=unidiomatic-typecheck</span>

  <span class="n">handle_data</span> <span class="o">=</span> <span class="n">pywrap_tensorflow</span><span class="o">.</span><span class="n">GetHandleShapeAndType</span><span class="p">(</span>
      <span class="n">graph_op</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span> <span class="n">graph_op</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">())</span>  <span class="c1"># pylint: disable=protected-access</span>

  <span class="k">return</span> <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="o">.</span><span class="n">FromString</span><span class="p">(</span>
      <span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="n">handle_data</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Get the data handle from the Tensor `handle`.&quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">handle</span><span class="o">.</span><span class="n">_handle_data</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">get_resource_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_set_handle_shapes_and_types</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">handle_data</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sets the shape inference result HandleData on tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: A `Tensor` or `EagerTensor`.</span>
<span class="sd">    handle_data: A `CppShapeInferenceResult.HandleData`.</span>
<span class="sd">    graph_mode: A python bool.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="n">handle_data</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">graph_mode</span><span class="p">:</span>
    <span class="k">return</span>

  <span class="c1"># Not an EagerTensor, so a graph tensor.</span>
  <span class="n">shapes</span><span class="p">,</span> <span class="n">types</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">pair</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">pair</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">])</span>
  <span class="n">ranks</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="o">.</span><span class="n">unknown_rank</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>
  <span class="n">shapes</span> <span class="o">=</span> <span class="p">[[</span><span class="n">d</span><span class="o">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">s</span><span class="o">.</span><span class="n">dim</span><span class="p">]</span>  <span class="c1"># pylint: disable=g-complex-comprehension</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">s</span><span class="o">.</span><span class="n">unknown_rank</span> <span class="k">else</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">]</span>
  <span class="n">pywrap_tensorflow</span><span class="o">.</span><span class="n">TF_GraphSetOutputHandleShapesAndTypes_wrapper</span><span class="p">(</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_op</span><span class="o">.</span><span class="n">_graph</span><span class="o">.</span><span class="n">_c_graph</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_as_tf_output</span><span class="p">(),</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">shapes</span><span class="p">,</span> <span class="n">ranks</span><span class="p">,</span> <span class="n">types</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_combine_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Concats HandleData from tensors `handle` and `initial_value`.</span>

<span class="sd">  Args:</span>
<span class="sd">    handle: A `Tensor` of dtype `resource`.</span>
<span class="sd">    initial_value: A `Tensor`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A `CppShapeInferenceResult.HandleData`.  If `initial_value` has dtype</span>
<span class="sd">    `variant`, the `HandleData` contains the concatenation of the shape_and_type</span>
<span class="sd">    from both `handle` and `initial_value`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    RuntimeError: If handle, which was returned by VarHandleOp, either has</span>
<span class="sd">      no handle data, or its len(handle_data.shape_and_type) != 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">assert</span> <span class="n">handle</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">resource</span>

  <span class="n">variable_handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">variable_handle_data</span>

  <span class="n">extra_handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">extra_handle_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">extra_handle_data</span><span class="o">.</span><span class="n">is_set</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">variable_handle_data</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">or</span> <span class="ow">not</span> <span class="n">variable_handle_data</span><span class="o">.</span><span class="n">is_set</span>
        <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">variable_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
          <span class="s2">&quot;Expected VarHandleOp to return a length==1 shape_and_type, &quot;</span>
          <span class="s2">&quot;but saw: &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">variable_handle_data</span><span class="p">,))</span>
    <span class="n">variable_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
        <span class="n">extra_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">variable_handle_data</span>


<span class="k">def</span> <span class="nf">variable_handle_from_shape_and_dtype</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">extra_handle_data</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Create a new variable handle, optionally copying in `extra_handle_data`.&quot;&quot;&quot;</span>
  <span class="n">container</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_container</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">container</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">container</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
  <span class="n">handle</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_handle_op</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                                   <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
                                                   <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">container</span><span class="o">=</span><span class="n">container</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">extra_handle_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">extra_handle_data</span> <span class="o">=</span> <span class="n">handle</span>
  <span class="k">if</span> <span class="n">graph_mode</span><span class="p">:</span>
    <span class="n">full_handle_data</span> <span class="o">=</span> <span class="n">_combine_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">extra_handle_data</span><span class="p">)</span>
    <span class="n">_set_handle_shapes_and_types</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">full_handle_data</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">handle</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># We do not want two distinct ResourceVariable objects for the same</span>
    <span class="c1"># underlying resource in the runtime.</span>
    <span class="c1"># When in eager mode, explicitly ensure so here. When in graph mode, it&#39;s</span>
    <span class="c1"># ensured by always generating different variable names.</span>
    <span class="n">exists</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>

    <span class="c1"># We create an assert Op instead of checking right away in order to be</span>
    <span class="c1"># compatible with ASYNC execution mode. Further, since not all devices</span>
    <span class="c1"># support string tensors, we encode the assertion string in the Op name</span>
    <span class="n">gen_logging_ops</span><span class="o">.</span><span class="n">_assert</span><span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">math_ops</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">exists</span><span class="p">),</span> <span class="p">[</span><span class="n">exists</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;EagerVariableNameReuse&quot;</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">graph_mode</span><span class="p">(),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span> <span class="k">as</span> <span class="n">graph</span><span class="p">:</span>
      <span class="n">h</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_handle_op</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                                  <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
                                                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                                                  <span class="n">container</span><span class="o">=</span><span class="n">container</span><span class="p">)</span>

      <span class="c1"># Tensor._handle_data contains information for the shape-inference code to</span>
      <span class="c1"># know the shape and dtype of the variable pointed to by a handle. Since</span>
      <span class="c1"># shape inference doesn&#39;t run in eager mode we copy this data here for</span>
      <span class="c1"># when the handle is captured by an eager mode function.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">full_handle_data</span> <span class="o">=</span> <span class="n">_combine_handle_data</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">extra_handle_data</span><span class="p">)</span>
      <span class="n">_set_handle_shapes_and_types</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">full_handle_data</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">)</span>
      <span class="c1"># pylint: enable=protected-access</span>
    <span class="c1"># Clean up op-&gt;graph-&gt;op reference cycles.</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">dismantle_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">handle</span>


<span class="k">def</span> <span class="nf">eager_safe_variable_handle</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span>
                               <span class="n">graph_mode</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a variable handle with information to do shape inference.</span>

<span class="sd">  The dtype is read from `initial_value` and stored in the returned</span>
<span class="sd">  resource tensor&#39;s handle data.</span>

<span class="sd">  If `initial_value.dtype == tf.variant`, we additionally extract the handle</span>
<span class="sd">  data (if any) from `initial_value` and append it to the `handle_data`.</span>
<span class="sd">  In this case, the returned tensor&#39;s handle data is in the form</span>

<span class="sd">  ```</span>
<span class="sd">  is_set: true</span>
<span class="sd">  shape_and_type {</span>
<span class="sd">    shape {</span>
<span class="sd">      // initial_value.shape</span>
<span class="sd">    }</span>
<span class="sd">    dtype: DT_VARIANT</span>
<span class="sd">  }</span>
<span class="sd">  shape_and_type {</span>
<span class="sd">    // handle_data(initial_value).shape_and_type[0]</span>
<span class="sd">  }</span>
<span class="sd">  shape_and_type {</span>
<span class="sd">    // handle_data(initial_value).shape_and_type[1]</span>
<span class="sd">  }</span>
<span class="sd">  ...</span>
<span class="sd">  ```</span>

<span class="sd">  Ops that read from this tensor, such as `ReadVariableOp` and</span>
<span class="sd">  `AssignVariableOp`, know that `handle_data(handle).shape_and_type[1:]`</span>
<span class="sd">  correspond to the handle data of the variant(s) stored in the Variable.</span>

<span class="sd">  Args:</span>
<span class="sd">    initial_value: A `Tensor`.</span>
<span class="sd">    shape: The shape of the handle data. Can be `TensorShape(None)`</span>
<span class="sd">      (i.e. unknown shape).</span>
<span class="sd">    shared_name: A string.</span>
<span class="sd">    name: A string.</span>
<span class="sd">    graph_mode: A python bool.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The handle, a `Tensor` of type `resource`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>
  <span class="k">return</span> <span class="n">variable_handle_from_shape_and_dtype</span><span class="p">(</span>
      <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">graph_mode</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">)</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_handle_graph</span><span class="p">(</span><span class="n">handle</span><span class="p">):</span>
  <span class="c1"># Note: might have an eager tensor but not be executing eagerly when building</span>
  <span class="c1"># functions.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">)</span>
      <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">has_default_graph</span><span class="p">()):</span>
    <span class="k">yield</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">handle</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
      <span class="k">yield</span>


<span class="k">class</span> <span class="nc">EagerResourceDeleter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;An object which cleans up a resource handle.</span>

<span class="sd">  An alternative to defining a __del__ method on an object. The intended use is</span>
<span class="sd">  that ResourceVariables or other objects with resource handles will maintain a</span>
<span class="sd">  single reference to this object. When the parent object is collected, this</span>
<span class="sd">  object will be too. Even if the parent object is part of a reference cycle,</span>
<span class="sd">  the cycle will be collectable.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">handle_device</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="p">(</span><span class="s2">&quot;Passed handle=</span><span class="si">%s</span><span class="s2"> to EagerResourceDeleter. Was expecting a handle &quot;</span>
           <span class="s2">&quot;Tensor.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle</span><span class="p">,)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">handle</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_device</span> <span class="o">=</span> <span class="n">handle_device</span>
    <span class="c1"># This is held since the __del__ function runs an op, and if the context()</span>
    <span class="c1"># is collected before this object, there will be a segfault when running the</span>
    <span class="c1"># op.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">context</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Resources follow object-identity when executing eagerly, so it is safe to</span>
    <span class="c1"># delete the resource we have a handle to.</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="c1"># This resource was created in eager mode. However, this destructor may be</span>
      <span class="c1"># running in graph mode (especially during unit tests). To clean up</span>
      <span class="c1"># successfully, we switch back into eager mode temporarily.</span>
      <span class="k">with</span> <span class="n">context</span><span class="o">.</span><span class="n">eager_mode</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle_device</span><span class="p">):</span>
          <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">destroy_resource_op</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">ignore_lookup_error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
      <span class="c1"># Suppress some exceptions, mainly for the case when we&#39;re running on</span>
      <span class="c1"># module deletion. Things that can go wrong include the context module</span>
      <span class="c1"># already being unloaded, self._handle._handle_data no longer being</span>
      <span class="c1"># valid, and so on. Printing warnings in these cases is silly</span>
      <span class="c1"># (exceptions raised from __del__ are printed as warnings to stderr).</span>
      <span class="k">pass</span>  <span class="c1"># &#39;NoneType&#39; object is not callable when the handle has been</span>
      <span class="c1"># partially unloaded.</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
      <span class="k">pass</span>  <span class="c1"># &#39;NoneType&#39; object has no attribute &#39;eager_mode&#39; when context has</span>
      <span class="c1"># been unloaded. Will catch other module unloads as well.</span>


<span class="k">def</span> <span class="nf">shape_safe_assign_variable_handle</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Helper that checks shape compatibility and assigns variable.&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="n">handle</span><span class="p">):</span>
    <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
  <span class="n">shape</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">value_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span>
                                                      <span class="n">value_tensor</span><span class="p">,</span>
                                                      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
    <span class="c1"># For DT_VARIANT types, the handle&#39;s shape_and_type[1:] stores the</span>
    <span class="c1"># variant&#39;s handle data.  Extract it.</span>
    <span class="n">handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">is_set</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">tensor</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="p">(</span>
              <span class="n">is_set</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">shape_and_type</span><span class="o">=</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>


<span class="k">def</span> <span class="nf">variable_accessed</span><span class="p">(</span><span class="n">variable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Records that `variable` was accessed for the tape and FuncGraph.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">(),</span> <span class="s2">&quot;watch_variable&quot;</span><span class="p">):</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">watch_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">variable</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">variable_accessed</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BaseResourceVariable</span><span class="p">(</span><span class="n">variables</span><span class="o">.</span><span class="n">VariableV1</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A python variable from an existing handle.&quot;&quot;&quot;</span>

  <span class="nd">@deprecated_args</span><span class="p">(</span>
      <span class="kc">None</span><span class="p">,</span>
      <span class="s2">&quot;If using Keras pass *_constraint arguments to layers.&quot;</span><span class="p">,</span>
      <span class="s2">&quot;constraint&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">handle</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">unique_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">handle_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">graph_element</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">initializer_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">is_initialized_op</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">cached_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">save_slice_info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">handle_deleter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="o">**</span><span class="n">unused_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable from a handle.</span>

<span class="sd">    Args:</span>
<span class="sd">      trainable: If `True`, GradientTapes automatically watch uses of this</span>
<span class="sd">        Variable.</span>
<span class="sd">      shape: The variable&#39;s shape.</span>
<span class="sd">      dtype: The variable&#39;s dtype.</span>
<span class="sd">      handle: The variable&#39;s handle</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      distribute_strategy: The distribution strategy this variable was created</span>
<span class="sd">        under.</span>
<span class="sd">      name: The name for this variable.</span>
<span class="sd">      unique_id: Internal. Unique ID for this variable&#39;s handle.</span>
<span class="sd">      handle_name: The name for the variable&#39;s handle.</span>
<span class="sd">      graph_element: Optional, required only in session.run-mode. Pre-created</span>
<span class="sd">        tensor which reads this variable&#39;s value.</span>
<span class="sd">      initial_value: Optional. Variable&#39;s initial value.</span>
<span class="sd">      initializer_op: Operation which assigns the variable&#39;s initial value.</span>
<span class="sd">      is_initialized_op: Pre-created operation to check whether this variable</span>
<span class="sd">        is initialized.</span>
<span class="sd">      cached_value: Pre-created operation to read this variable in a specific</span>
<span class="sd">        device.</span>
<span class="sd">      save_slice_info: Metadata for variable partitioning.</span>
<span class="sd">      handle_deleter: EagerResourceDeleter responsible for cleaning up the</span>
<span class="sd">        handle.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span> <span class="o">=</span> <span class="n">synchronization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="n">save_slice_info</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">initial_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">initializer_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_initialized_op</span> <span class="o">=</span> <span class="n">is_initialized_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">graph_element</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="n">cached_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_distribute_strategy</span> <span class="o">=</span> <span class="n">distribute_strategy</span>
    <span class="c1"># Store the graph key so optimizers know how to only retrieve variables from</span>
    <span class="c1"># this graph. Guaranteed to be the same as the eager graph_key.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_key</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_graph_key</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">as_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">handle</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">graph_element</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span> <span class="o">=</span> <span class="n">unique_id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span> <span class="o">=</span> <span class="n">handle_name</span> <span class="o">+</span> <span class="s2">&quot;:0&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="n">constraint</span>
    <span class="c1"># After the handle has been created, set up a way to clean it up when</span>
    <span class="c1"># executing eagerly. We&#39;ll hold the only reference to the deleter, so that</span>
    <span class="c1"># when this object is garbage collected the deleter will be too. This</span>
    <span class="c1"># means ResourceVariables can be part of reference cycles without those</span>
    <span class="c1"># cycles being uncollectable.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">handle_deleter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">handle_deleter</span> <span class="o">=</span> <span class="n">EagerResourceDeleter</span><span class="p">(</span>
            <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">handle_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_deleter</span> <span class="o">=</span> <span class="n">handle_deleter</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_shape_as_list</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;&lt;tf.Variable &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">, numpy=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">numpy_text</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">(),</span> <span class="n">is_repr</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;&lt;tf.Variable &#39;</span><span class="si">%s</span><span class="s2">&#39; shape=</span><span class="si">%s</span><span class="s2"> dtype=</span><span class="si">%s</span><span class="s2">&gt;&quot;</span> <span class="o">%</span> <span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
  <span class="k">def</span> <span class="nf">_assign_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Makes assignments depend on the cached value, if any.</span>

<span class="sd">    This prevents undefined behavior with reads not ordered wrt writes.</span>

<span class="sd">    Yields:</span>
<span class="sd">      None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span><span class="p">]):</span>
        <span class="k">yield</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">yield</span>

  <span class="k">def</span> <span class="nf">__nonzero__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__bool__</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span>

  <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="s2">&quot;__deepcopy__() is only available when eager execution is enabled.&quot;</span><span class="p">)</span>
    <span class="n">copied_variable</span> <span class="o">=</span> <span class="n">ResourceVariable</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">(),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_name</span> <span class="o">+</span> <span class="s2">&quot;_copy&quot;</span><span class="p">,</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_distribute_strategy</span><span class="p">)</span>
    <span class="n">memo</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">copied_variable</span>
    <span class="k">return</span> <span class="n">copied_variable</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The dtype of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The device this variable is on.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">device</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The `Graph` of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">graph</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The name of the handle for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The shape of this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

  <span class="k">def</span> <span class="nf">_shape_as_list</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dim</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">_shape_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape_as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op responsible for initializing this variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Calling create is not supported when eager execution&quot;</span>
                         <span class="s2">&quot; is enabled.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">handle</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The handle by which this variable can be accessed.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span>

  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A cached operation which reads the value of this variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_as_graph_element</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Conversion function for Graph.as_graph_element().&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op responsible for initializing this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initial_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Tensor used as the initial value for the variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;initial_value not supported in EAGER mode.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the constraint function associated with this variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The constraint function that was passed to the variable constructor.</span>
<span class="sd">      Can be `None` if no constraint was passed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">synchronization</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">aggregation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span>

  <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates and returns the value of this variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Trying to eval in EAGER mode&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">numpy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;numpy() is only available when eager execution is enabled.&quot;</span><span class="p">)</span>

  <span class="nd">@deprecated</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Prefer Dataset.range instead.&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Increments this variable until it reaches `limit`.</span>

<span class="sd">    When that Op is run it tries to increment the variable by `1`. If</span>
<span class="sd">    incrementing the variable would bring it above `limit` then the Op raises</span>
<span class="sd">    the exception `OutOfRangeError`.</span>

<span class="sd">    If no error is raised, the Op outputs the value of the variable before</span>
<span class="sd">    the increment.</span>

<span class="sd">    This is essentially a shortcut for `count_up_to(self, limit)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      limit: value at which incrementing the variable raises an error.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the variable value before the increment. If no</span>
<span class="sd">      other Op modifies this variable, the values produced will all be</span>
<span class="sd">      distinct.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_count_up_to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">,</span>
                                              <span class="n">T</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
                                                        <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
    <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="c1"># Note that if a control flow context is active the input of the read op</span>
      <span class="c1"># might not actually be the handle. This line bypasses it.</span>
      <span class="n">tape</span><span class="o">.</span><span class="n">record_operation</span><span class="p">(</span>
          <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">result</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">],</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">read_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an op which reads the value of this variable.</span>

<span class="sd">    Should be used when there are multiple reads, or when it is desirable to</span>
<span class="sd">    read the value only after some condition is true.</span>

<span class="sd">    Returns:</span>
<span class="sd">     the read operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Read&quot;</span><span class="p">):</span>
      <span class="c1"># Ensure we read the variable in the same device as the handle.</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>
    <span class="c1"># Return an identity so it can get placed on whatever device the context</span>
    <span class="c1"># specifies instead of the device where the variable is.</span>
    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">sparse_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reads the value of this variable sparsely, using `gather`.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Gather&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_gather</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">==</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">variant</span><span class="p">:</span>
        <span class="c1"># For DT_VARIANT types, the handle&#39;s shape_and_type[1:] stores the</span>
        <span class="c1"># variant&#39;s handle data.  Extract it.</span>
        <span class="n">handle_data</span> <span class="o">=</span> <span class="n">get_eager_safe_handle_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">handle_data</span><span class="o">.</span><span class="n">is_set</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
          <span class="n">value</span><span class="o">.</span><span class="n">_handle_data</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># pylint: disable=protected-access</span>
              <span class="n">cpp_shape_inference_pb2</span><span class="o">.</span><span class="n">CppShapeInferenceResult</span><span class="o">.</span><span class="n">HandleData</span><span class="p">(</span>
                  <span class="n">is_set</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">shape_and_type</span><span class="o">=</span><span class="n">handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reads the value of this variable sparsely, using `gather_nd`.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;GatherNd&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
        <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_gather_nd</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">to_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a `ResourceVariable` to a `VariableDef` protocol buffer.</span>

<span class="sd">    Args:</span>
<span class="sd">      export_scope: Optional `string`. Name scope to remove.</span>

<span class="sd">    Raises:</span>
<span class="sd">      RuntimeError: If run in EAGER mode.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `VariableDef` protocol buffer, or `None` if the `Variable` is not</span>
<span class="sd">      in the specified name scope.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;to_proto not supported in EAGER mode.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">export_scope</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">export_scope</span><span class="p">):</span>
      <span class="n">var_def</span> <span class="o">=</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">()</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">variable_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                   <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># This is inside an if-statement for backwards compatibility, since</span>
        <span class="c1"># self._initial_value might be None for variables constructed from old</span>
        <span class="c1"># protos.</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">initial_value_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">initializer_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                      <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">snapshot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">export_scope</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Store the graph_element here</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">snapshot_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">strip_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">export_scope</span><span class="p">)</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">is_resource</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">synchronization</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">synchronization</span><span class="o">.</span><span class="n">value</span>
      <span class="n">var_def</span><span class="o">.</span><span class="n">aggregation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation</span><span class="o">.</span><span class="n">value</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span><span class="p">:</span>
        <span class="n">var_def</span><span class="o">.</span><span class="n">save_slice_info_def</span><span class="o">.</span><span class="n">MergeFrom</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">var_def</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">from_proto</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;from_proto not supported in EAGER mode.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ResourceVariable</span><span class="p">(</span>
        <span class="n">variable_def</span><span class="o">=</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">set_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unsupported.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;ResourceVariable does not implement set_shape()&quot;</span><span class="p">)</span>

  <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">100</span>

  <span class="k">def</span> <span class="nf">is_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks whether a resource variable has been initialized.</span>

<span class="sd">    Outputs boolean scalar indicating whether the tensor has been initialized.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: A name for the operation (optional).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` of type `bool`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">assign_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts a value from this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to subtract from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name to use for the operation.</span>
<span class="sd">      read_value: A `bool`. Whether to read and return the new value of the</span>
<span class="sd">          variable or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      If `read_value` is `True`, this method will return the new value of the</span>
<span class="sd">      variable after the assignment has completed. Otherwise, when in graph mode</span>
<span class="sd">      it will return the `Operation` that does the assignment, and when in eager</span>
<span class="sd">      mode it will return `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO(apassos): this here and below is not atomic. Consider making it</span>
    <span class="c1"># atomic if there&#39;s a way to do so without a performance cost for those who</span>
    <span class="c1"># don&#39;t need it.</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_dependencies</span><span class="p">():</span>
      <span class="n">assign_sub_op</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_sub_variable_op</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">assign_sub_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assign_sub_op</span>

  <span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds a value to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      delta: A `Tensor`. The value to add to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: The name to use for the operation.</span>
<span class="sd">      read_value: A `bool`. Whether to read and return the new value of the</span>
<span class="sd">          variable or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      If `read_value` is `True`, this method will return the new value of the</span>
<span class="sd">      variable after the assignment has completed. Otherwise, when in graph mode</span>
<span class="sd">      it will return the `Operation` that does the assignment, and when in eager</span>
<span class="sd">      mode it will return `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_dependencies</span><span class="p">():</span>
      <span class="n">assign_add_op</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_add_variable_op</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">assign_add_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assign_add_op</span>

  <span class="k">def</span> <span class="nf">_lazy_read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
    <span class="n">variable_accessed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_UnreadVariable</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
        <span class="n">in_graph_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">,</span>
        <span class="n">deleter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle_deleter</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">parent_op</span><span class="o">=</span><span class="n">op</span><span class="p">,</span> <span class="n">unique_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">read_value</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns a new value to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      value: A `Tensor`. The new value for this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the assignment.</span>
<span class="sd">      name: The name to use for the assignment.</span>
<span class="sd">      read_value: A `bool`. Whether to read and return the new value of the</span>
<span class="sd">          variable or not.</span>

<span class="sd">    Returns:</span>
<span class="sd">      If `read_value` is `True`, this method will return the new value of the</span>
<span class="sd">      variable after the assignment has completed. Otherwise, when in graph mode</span>
<span class="sd">      it will return the `Operation` that does the assignment, and when in eager</span>
<span class="sd">      mode it will return `None`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note: not depending on the cached value here since this can used to</span>
    <span class="c1"># initialize the variable.</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
      <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">value_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="n">assign_op</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">read_value</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">assign_op</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">assign_op</span>

  <span class="k">def</span> <span class="nf">__reduce__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># The implementation mirrors that of __deepcopy__.</span>
    <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
        <span class="n">ResourceVariable</span><span class="p">,</span>
        <span class="n">initial_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shared_name</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_distribute_strategy</span><span class="p">),</span> <span class="p">()</span>

  <span class="k">def</span> <span class="nf">scatter_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts `tf.IndexedSlices` from this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be subtracted from this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be added to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered addition has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the max of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of max</span>
<span class="sd">        with this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered maximization has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_max</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Updates this variable with the min of `tf.IndexedSlices` and itself.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to use as an argument of min</span>
<span class="sd">        with this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered minimization has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_min</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multiply this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to multiply this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered multiplication has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_mul</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Divide this variable by `tf.IndexedSlices`.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to divide this variable by.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered division has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_div</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">resource_scatter_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">batch_scatter_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns `tf.IndexedSlices` to this variable batch-wise.</span>

<span class="sd">    Analogous to `batch_gather`. This assumes that this variable and the</span>
<span class="sd">    sparse_delta IndexedSlices have a series of leading dimensions that are the</span>
<span class="sd">    same for all of them, and the updates are performed on the last dimension of</span>
<span class="sd">    indices. In other words, the dimensions should be the following:</span>

<span class="sd">    `num_prefix_dims = sparse_delta.indices.ndims - 1`</span>
<span class="sd">    `batch_dim = num_prefix_dims + 1`</span>
<span class="sd">    `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[</span>
<span class="sd">         batch_dim:]`</span>

<span class="sd">    where</span>

<span class="sd">    `sparse_delta.updates.shape[:num_prefix_dims]`</span>
<span class="sd">    `== sparse_delta.indices.shape[:num_prefix_dims]`</span>
<span class="sd">    `== var.shape[:num_prefix_dims]`</span>

<span class="sd">    And the operation performed can be expressed as:</span>

<span class="sd">    `var[i_1, ..., i_n,</span>
<span class="sd">         sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[</span>
<span class="sd">            i_1, ..., i_n, j]`</span>

<span class="sd">    When sparse_delta.indices is a 1D tensor, this operation is equivalent to</span>
<span class="sd">    `scatter_update`.</span>

<span class="sd">    To avoid this operation one can looping over the first `ndims` of the</span>
<span class="sd">    variable and using `scatter_update` on the subtensors that result of slicing</span>
<span class="sd">    the first dimension. This is a valid option for `ndims = 1`, but less</span>
<span class="sd">    efficient than this implementation.</span>

<span class="sd">    Args:</span>
<span class="sd">      sparse_delta: `tf.IndexedSlices` to be assigned to this variable.</span>
<span class="sd">      use_locking: If `True`, use locking during the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: if `sparse_delta` is not an `IndexedSlices`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sparse_delta</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;sparse_delta is not IndexedSlices: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">sparse_delta</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">state_ops</span><span class="o">.</span><span class="n">batch_scatter_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span> <span class="n">sparse_delta</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
        <span class="n">use_locking</span><span class="o">=</span><span class="n">use_locking</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse subtraction to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = ref.scatter_nd_sub(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, -9, 3, -6, -6, 6, 7, -4]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_sub</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse addition to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        add = ref.scatter_nd_add(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(add)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, 13, 3, 14, 14, 6, 7, 20]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_add</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">scatter_nd_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Applies sparse assignment to individual values or slices in a Variable.</span>

<span class="sd">    `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.</span>

<span class="sd">    `indices` must be integer tensor, containing indices into `ref`.</span>
<span class="sd">    It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 &lt; K &lt;= P`.</span>

<span class="sd">    The innermost dimension of `indices` (with length `K`) corresponds to</span>
<span class="sd">    indices into elements (if `K = P`) or slices (if `K &lt; P`) along the `K`th</span>
<span class="sd">    dimension of `ref`.</span>

<span class="sd">    `updates` is `Tensor` of rank `Q-1+P-K` with shape:</span>

<span class="sd">    ```</span>
<span class="sd">    [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].</span>
<span class="sd">    ```</span>

<span class="sd">    For example, say we want to add 4 scattered elements to a rank-1 tensor to</span>
<span class="sd">    8 elements. In Python, that update would look like this:</span>

<span class="sd">    ```python</span>
<span class="sd">        ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="sd">        indices = tf.constant([[4], [3], [1] ,[7]])</span>
<span class="sd">        updates = tf.constant([9, 10, 11, 12])</span>
<span class="sd">        op = ref.scatter_nd_update(indices, updates)</span>
<span class="sd">        with tf.compat.v1.Session() as sess:</span>
<span class="sd">          print sess.run(op)</span>
<span class="sd">    ```</span>

<span class="sd">    The resulting update to ref would look like this:</span>

<span class="sd">        [1, 11, 3, 10, 9, 6, 7, 12]</span>

<span class="sd">    See `tf.scatter_nd` for more details about how to make updates to</span>
<span class="sd">    slices.</span>

<span class="sd">    Args:</span>
<span class="sd">      indices: The indices to be used in the operation.</span>
<span class="sd">      updates: The values to be used in the operation.</span>
<span class="sd">      name: the name of the operation.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A `Tensor` that will hold the new value of this variable after</span>
<span class="sd">      the scattered subtraction has completed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span><span class="n">gen_state_ops</span><span class="o">.</span><span class="n">resource_scatter_nd_update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">updates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_strided_slice_assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">begin_mask</span><span class="p">,</span>
                            <span class="n">end_mask</span><span class="p">,</span> <span class="n">ellipsis_mask</span><span class="p">,</span> <span class="n">new_axis_mask</span><span class="p">,</span>
                            <span class="n">shrink_axis_mask</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">_handle_graph</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_assign_dependencies</span><span class="p">():</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lazy_read</span><span class="p">(</span>
          <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">resource_strided_slice_assign</span><span class="p">(</span>
              <span class="n">ref</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span>
              <span class="n">begin</span><span class="o">=</span><span class="n">begin</span><span class="p">,</span>
              <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">,</span>
              <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
              <span class="n">value</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
              <span class="n">begin_mask</span><span class="o">=</span><span class="n">begin_mask</span><span class="p">,</span>
              <span class="n">end_mask</span><span class="o">=</span><span class="n">end_mask</span><span class="p">,</span>
              <span class="n">ellipsis_mask</span><span class="o">=</span><span class="n">ellipsis_mask</span><span class="p">,</span>
              <span class="n">new_axis_mask</span><span class="o">=</span><span class="n">new_axis_mask</span><span class="p">,</span>
              <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="n">shrink_axis_mask</span><span class="p">))</span>

  <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Non-integer variable can&#39;t be converted to integer.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">_dense_var_to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">name</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Incompatible type conversion requested to type </span><span class="si">{!r}</span><span class="s2"> for variable &quot;</span>
          <span class="s2">&quot;of type </span><span class="si">{!r}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">as_ref</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_value</span><span class="p">()</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">()</span>

  <span class="k">def</span> <span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable += value not supported. Use &quot;</span>
                       <span class="s2">&quot;variable.assign_add(value) to modify the variable &quot;</span>
                       <span class="s2">&quot;value and variable = variable + value to get a new &quot;</span>
                       <span class="s2">&quot;Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable -= value not supported. Use &quot;</span>
                       <span class="s2">&quot;variable.assign_sub(value) to modify the variable &quot;</span>
                       <span class="s2">&quot;value and variable = variable - value to get a new &quot;</span>
                       <span class="s2">&quot;Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable *= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var * value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var * value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__idiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable /= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var / value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var / value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable /= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var / value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var / value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__irealdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable /= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var / value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var / value` to get a new Tensor object.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unused_other</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Variable **= value not supported. Use &quot;</span>
                       <span class="s2">&quot;`var.assign(var ** value)` to modify the variable or &quot;</span>
                       <span class="s2">&quot;`var = var ** value` to get a new Tensor object.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResourceVariable</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Variable based on resource handles.</span>

<span class="sd">  See the [Variables How To](https://tensorflow.org/guide/variables)</span>
<span class="sd">  for a high level overview.</span>

<span class="sd">  A `ResourceVariable` allows you to maintain state across subsequent calls to</span>
<span class="sd">  session.run.</span>

<span class="sd">  The `ResourceVariable` constructor requires an initial value for the variable,</span>
<span class="sd">  which can be a `Tensor` of any type and shape. The initial value defines the</span>
<span class="sd">  type and shape of the variable. After construction, the type and shape of</span>
<span class="sd">  the variable are fixed. The value can be changed using one of the assign</span>
<span class="sd">  methods.</span>

<span class="sd">  Just like any `Tensor`, variables created with</span>
<span class="sd">  `tf.Variable(use_resource=True)` can be used as inputs for other Ops in the</span>
<span class="sd">  graph. Additionally, all the operators overloaded for the `Tensor` class are</span>
<span class="sd">  carried over to variables, so you can also add nodes to the graph by just</span>
<span class="sd">  doing arithmetic on variables.</span>

<span class="sd">  Unlike ref-based variable, a ResourceVariable has well-defined semantics. Each</span>
<span class="sd">  usage of a ResourceVariable in a TensorFlow graph adds a read_value operation</span>
<span class="sd">  to the graph. The Tensors returned by a read_value operation are guaranteed to</span>
<span class="sd">  see all modifications to the value of the variable which happen in any</span>
<span class="sd">  operation on which the read_value depends on (either directly, indirectly, or</span>
<span class="sd">  via a control dependency) and guaranteed to not see any modification to the</span>
<span class="sd">  value of the variable from operations that depend on the read_value operation.</span>
<span class="sd">  Updates from operations that have no dependency relationship to the read_value</span>
<span class="sd">  operation might or might not be visible to read_value.</span>

<span class="sd">  For example, if there is more than one assignment to a ResourceVariable in</span>
<span class="sd">  a single session.run call there is a well-defined value for each operation</span>
<span class="sd">  which uses the variable&#39;s value if the assignments and the read are connected</span>
<span class="sd">  by edges in the graph. Consider the following example, in which two writes</span>
<span class="sd">  can cause tf.Variable and tf.ResourceVariable to behave differently:</span>

<span class="sd">  ```python</span>
<span class="sd">  a = tf.Variable(1.0, use_resource=True)</span>
<span class="sd">  a.initializer.run()</span>

<span class="sd">  assign = a.assign(2.0)</span>
<span class="sd">  with tf.control_dependencies([assign]):</span>
<span class="sd">    b = a.read_value()</span>
<span class="sd">  with tf.control_dependencies([b]):</span>
<span class="sd">    other_assign = a.assign(3.0)</span>
<span class="sd">  with tf.control_dependencies([other_assign]):</span>
<span class="sd">    # Will print 2.0 because the value was read before other_assign ran. If</span>
<span class="sd">    # `a` was a tf.Variable instead, 2.0 or 3.0 could be printed.</span>
<span class="sd">    tf.compat.v1.Print(b, [b]).eval()</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
               <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">validate_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># pylint: disable=unused-argument</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">variable_def</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called.</span>
<span class="sd">        (Note that initializer functions from init_ops.py must first be bound</span>
<span class="sd">         to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, the default, also adds the variable to the graph</span>
<span class="sd">        collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as</span>
<span class="sd">        the default list of variables to use by the `Optimizer` classes.</span>
<span class="sd">        Defaults to `True`, unless `synchronization` is set to `ON_READ`, in</span>
<span class="sd">        which case it defaults to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      validate_shape: Ignored. Provided for compatibility with tf.Variable.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If None, either the datatype will be kept (if initial_value is</span>
<span class="sd">        a Tensor) or float32 will be used (if it is a Python object convertible</span>
<span class="sd">        to a Tensor).</span>
<span class="sd">      variable_def: `VariableDef` protocol buffer. If not None, recreates the</span>
<span class="sd">        `ResourceVariable` object with its contents. `variable_def` and other</span>
<span class="sd">        arguments (except for import_scope) are mutually exclusive.</span>
<span class="sd">      import_scope: Optional `string`. Name scope to add to the</span>
<span class="sd">        ResourceVariable. Only used when `variable_def` is provided.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      distribute_strategy: The tf.distribute.Strategy this variable is being</span>
<span class="sd">        created inside of.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>

<span class="sd">    @compatibility(eager)</span>
<span class="sd">    When Eager Execution is enabled, the default for the `collections` argument</span>
<span class="sd">    is `None`, which signifies that this `Variable` will not be added to any</span>
<span class="sd">    collections.</span>
<span class="sd">    @end_compatibility</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;variable_def and initial_value are mutually &quot;</span>
                         <span class="s2">&quot;exclusive.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Creating ResourceVariable from variable_def is &quot;</span>
                         <span class="s2">&quot;not supported when eager execution is enabled.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_proto</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_from_args</span><span class="p">(</span>
          <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">collections</span><span class="o">=</span><span class="n">collections</span><span class="p">,</span>
          <span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
          <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_init_from_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">initial_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">collections</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates a variable.</span>

<span class="sd">    Args:</span>
<span class="sd">      initial_value: A `Tensor`, or Python object convertible to a `Tensor`,</span>
<span class="sd">        which is the initial value for the Variable. The initial value must have</span>
<span class="sd">        a shape specified unless `validate_shape` is set to False. Can also be a</span>
<span class="sd">        callable with no argument that returns the initial value when called.</span>
<span class="sd">        (Note that initializer functions from init_ops.py must first be bound</span>
<span class="sd">         to a shape before being used here.)</span>
<span class="sd">      trainable: If `True`, the default, also adds the variable to the graph</span>
<span class="sd">        collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as</span>
<span class="sd">        the default list of variables to use by the `Optimizer` classes.</span>
<span class="sd">        Defaults to `True`, unless `synchronization` is set to `ON_READ`, in</span>
<span class="sd">        which case it defaults to `False`.</span>
<span class="sd">      collections: List of graph collections keys. The new variable is added to</span>
<span class="sd">        these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      dtype: If set, initial_value will be converted to the given type.</span>
<span class="sd">        If None, either the datatype will be kept (if initial_value is</span>
<span class="sd">       a Tensor) or float32 will be used (if it is a Python object convertible</span>
<span class="sd">       to a Tensor).</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      distribute_strategy: DistributionStrategy under which this variable</span>
<span class="sd">        was created.</span>
<span class="sd">      shape: (optional) The shape of this variable. If None, the shape of</span>
<span class="sd">        `initial_value` will be used. When setting this argument to</span>
<span class="sd">        `tf.TensorShape(None)` (representing an unspecified shape), the variable</span>
<span class="sd">        can be assigned with values of different shapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the initial value is not specified, or does not have a</span>
<span class="sd">        shape and `validate_shape` is `True`.</span>

<span class="sd">    @compatibility(eager)</span>
<span class="sd">    When Eager Execution is enabled, variables are never added to collections.</span>
<span class="sd">    It is not implicitly added to the `GLOBAL_VARIABLES` or</span>
<span class="sd">    `TRAINABLE_VARIABLES` collections, and the `collections` argument is</span>
<span class="sd">    ignored.</span>
<span class="sd">    @end_compatibility</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;initial_value must be specified.&quot;</span><span class="p">)</span>
    <span class="n">init_from_fn</span> <span class="o">=</span> <span class="n">callable</span><span class="p">(</span><span class="n">initial_value</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
        <span class="n">initial_value</span><span class="p">,</span> <span class="s2">&quot;graph&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">building_function</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tensor-typed variable initializers must either be &quot;</span>
                       <span class="s2">&quot;wrapped in an init_scope or callable &quot;</span>
                       <span class="s2">&quot;(e.g., `tf.Variable(lambda : &quot;</span>
                       <span class="s2">&quot;tf.truncated_normal([10, 40]))`) when building &quot;</span>
                       <span class="s2">&quot;functions. Please file a feature request if this &quot;</span>
                       <span class="s2">&quot;restriction inconveniences you.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">collections</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">set</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;collections argument to Variable constructor must be a list, tuple, &quot;</span>
          <span class="s2">&quot;or set. Got </span><span class="si">%s</span><span class="s2"> of type </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">collections</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">constraint</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `constraint` argument must be a callable.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial_value</span><span class="p">,</span> <span class="n">trackable</span><span class="o">.</span><span class="n">CheckpointInitialValue</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_initialize_trackable</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_update_uid</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">checkpoint_position</span><span class="o">.</span><span class="n">restore_uid</span>
      <span class="n">initial_value</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">wrapped_value</span>

    <span class="k">if</span> <span class="n">trainable</span> <span class="ow">and</span> <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">collections</span><span class="p">:</span>
      <span class="n">collections</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">collections</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="p">[]</span>
                          <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="p">[</span><span class="n">initial_value</span><span class="p">])</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="n">handle_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">handle_name</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="n">shared_name</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># When in eager mode use a uid for the shared_name, to prevent</span>
          <span class="c1"># accidental sharing.</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle_name</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">uid</span><span class="p">())</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shared_name</span><span class="p">()</span>
        <span class="c1"># Use attr_scope and device(None) to simulate the behavior of</span>
        <span class="c1"># colocate_with when the variable we want to colocate with doesn&#39;t</span>
        <span class="c1"># yet exist.</span>
        <span class="n">device_context_manager</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">ops</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="k">else</span> <span class="n">ops</span><span class="o">.</span><span class="n">NullContextmanager</span><span class="p">)</span>
        <span class="n">attr</span> <span class="o">=</span> <span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="p">(</span>
            <span class="nb">list</span><span class="o">=</span><span class="n">attr_value_pb2</span><span class="o">.</span><span class="n">AttrValue</span><span class="o">.</span><span class="n">ListValue</span><span class="p">(</span>
                <span class="n">s</span><span class="o">=</span><span class="p">[</span><span class="n">compat</span><span class="o">.</span><span class="n">as_bytes</span><span class="p">(</span><span class="s2">&quot;loc:@</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">handle_name</span><span class="p">)]))</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_attr_scope</span><span class="p">({</span><span class="s2">&quot;_class&quot;</span><span class="p">:</span> <span class="n">attr</span><span class="p">}):</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Initializer&quot;</span><span class="p">),</span> <span class="n">device_context_manager</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
            <span class="n">initial_value</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
                <span class="n">initial_value</span><span class="p">()</span> <span class="k">if</span> <span class="n">init_from_fn</span> <span class="k">else</span> <span class="n">initial_value</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial_value&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
              <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                  <span class="s2">&quot;The initial value&#39;s shape (</span><span class="si">%s</span><span class="s2">) is not compatible with &quot;</span>
                  <span class="s2">&quot;the explicitly supplied `shape` argument (</span><span class="si">%s</span><span class="s2">).&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">shape</span>
          <span class="n">handle</span> <span class="o">=</span> <span class="n">eager_safe_variable_handle</span><span class="p">(</span>
              <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
              <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
              <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
              <span class="n">graph_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">)</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="ow">and</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
            <span class="n">initial_value</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">_get_control_flow_context</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s2">&quot;Initializer for variable </span><span class="si">%s</span><span class="s2"> is from inside a control-flow &quot;</span>
              <span class="s2">&quot;construct, such as a loop or conditional. When creating a &quot;</span>
              <span class="s2">&quot;variable inside a loop or conditional, use a lambda as the &quot;</span>
              <span class="s2">&quot;initializer.&quot;</span> <span class="o">%</span> <span class="n">name</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">initial_value</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">base_dtype</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;IsInitialized&quot;</span><span class="p">):</span>
            <span class="n">is_initialized_op</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">var_is_initialized_op</span><span class="p">(</span><span class="n">handle</span><span class="p">))</span>
          <span class="k">if</span> <span class="n">initial_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># pylint: disable=g-backslash-continuation</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Assign&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">n</span><span class="p">,</span> \
                 <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> \
                 <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
              <span class="c1"># pylint: disable=protected-access</span>
              <span class="n">initializer_op</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span>
                      <span class="n">handle</span><span class="p">,</span>
                      <span class="n">variables</span><span class="o">.</span><span class="n">_try_guard_against_uninitialized_dependencies</span><span class="p">(</span>
                          <span class="n">name</span><span class="p">,</span>
                          <span class="n">initial_value</span><span class="p">),</span>
                      <span class="n">name</span><span class="o">=</span><span class="n">n</span><span class="p">))</span>
              <span class="c1"># pylint: enable=protected-access</span>
            <span class="c1"># pylint: enable=g-backslash-continuation</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Read&quot;</span><span class="p">):</span>
            <span class="c1"># Manually assign reads to the handle&#39;s device to avoid log</span>
            <span class="c1"># messages.</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
              <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
              <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">graph_element</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">if</span> <span class="n">caching_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
              <span class="c1"># Variables may be created in a tf.device() or ops.colocate_with()</span>
              <span class="c1"># context. At the same time, users would expect caching device to</span>
              <span class="c1"># be independent of this context, and/or would not expect the</span>
              <span class="c1"># current device context to be merged with the caching device</span>
              <span class="c1"># spec.  Therefore we reset the colocation stack before creating</span>
              <span class="c1"># the cached value. Note that resetting the colocation stack will</span>
              <span class="c1"># also reset the device stack.</span>
              <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">colocate_with</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_existing</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">caching_device</span><span class="p">):</span>
                  <span class="n">cached_value</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">cached_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">assign_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">initial_value</span><span class="p">)</span>
          <span class="n">is_initialized_op</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="n">initializer_op</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="k">if</span> <span class="n">caching_device</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">caching_device</span><span class="p">):</span>
              <span class="n">cached_value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span>
                  <span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
              <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">cached_value</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">cached_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="c1"># Eager variables are only added to collections if they are part of an</span>
          <span class="c1"># eager variable store (otherwise in an interactive session they would</span>
          <span class="c1"># hog memory and cause OOM). This is done in ops/variable_scope.py.</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">collections</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_STEP</span> <span class="ow">in</span> <span class="n">collections</span><span class="p">:</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collections</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_STEP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
      <span class="n">initial_value</span> <span class="o">=</span> <span class="n">initial_value</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="k">else</span> <span class="kc">None</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">ResourceVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
          <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span>
          <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
          <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span> <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">unique_id</span><span class="o">=</span><span class="n">unique_id</span><span class="p">,</span> <span class="n">handle_name</span><span class="o">=</span><span class="n">handle_name</span><span class="p">,</span>
          <span class="n">graph_element</span><span class="o">=</span><span class="n">graph_element</span><span class="p">,</span> <span class="n">initial_value</span><span class="o">=</span><span class="n">initial_value</span><span class="p">,</span>
          <span class="n">initializer_op</span><span class="o">=</span><span class="n">initializer_op</span><span class="p">,</span> <span class="n">is_initialized_op</span><span class="o">=</span><span class="n">is_initialized_op</span><span class="p">,</span>
          <span class="n">cached_value</span><span class="o">=</span><span class="n">cached_value</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_init_from_proto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_def</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initializes from `VariableDef` proto.&quot;&quot;&quot;</span>
    <span class="c1"># Note that init_from_proto is currently not supported in Eager mode.</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">is_resource</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Trying to restore Variable as ResourceVariable.&quot;</span><span class="p">)</span>

    <span class="c1"># Create from variable_def.</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">variable_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_unique_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle_name</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_op</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
        <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">initializer_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="c1"># Check whether initial_value_name exists for backwards compatibility.</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">variable_def</span><span class="p">,</span> <span class="s2">&quot;initial_value_name&quot;</span><span class="p">)</span> <span class="ow">and</span>
        <span class="n">variable_def</span><span class="o">.</span><span class="n">initial_value_name</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span><span class="n">variable_def</span><span class="o">.</span><span class="n">initial_value_name</span><span class="p">,</span>
                                 <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initial_value</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="p">,</span> <span class="n">trainable</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">variables</span><span class="o">.</span><span class="n">validate_synchronization_aggregation_trainable</span><span class="p">(</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">synchronization</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">variable_def</span><span class="o">.</span><span class="n">variable_name</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_synchronization</span> <span class="o">=</span> <span class="n">synchronization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation</span> <span class="o">=</span> <span class="n">aggregation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">snapshot_name</span><span class="p">:</span>
      <span class="n">snapshot</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">as_graph_element</span><span class="p">(</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">prepend_name_scope</span><span class="p">(</span>
              <span class="n">variable_def</span><span class="o">.</span><span class="n">snapshot_name</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="n">snapshot</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">while</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">snapshot</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cached_value</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="c1"># Legacy case for protos without the snapshot name; assume it&#39;s the</span>
      <span class="c1"># following.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph_element</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/Read/ReadVariableOp:0&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">variable_def</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;save_slice_info_def&quot;</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">SaveSliceInfo</span><span class="p">(</span>
          <span class="n">save_slice_info_def</span><span class="o">=</span><span class="n">variable_def</span><span class="o">.</span><span class="n">save_slice_info_def</span><span class="p">,</span>
          <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_save_slice_info</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_caching_device</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_constraint</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">UninitializedVariable</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A variable with no initializer.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">caching_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">extra_handle_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">distribute_strategy</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="o">**</span><span class="n">unused_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates the variable handle.</span>

<span class="sd">    Args:</span>
<span class="sd">      trainable: If `True`, GradientTapes automatically watch uses of this</span>
<span class="sd">        Variable.</span>
<span class="sd">      caching_device: Optional device string or function describing where the</span>
<span class="sd">        Variable should be cached for reading.  Defaults to the Variable&#39;s</span>
<span class="sd">        device.  If not `None`, caches on another device.  Typical use is to</span>
<span class="sd">        cache on the device where the Ops using the Variable reside, to</span>
<span class="sd">        deduplicate copying through `Switch` and other conditional statements.</span>
<span class="sd">      name: Optional name for the variable. Defaults to `&#39;Variable&#39;` and gets</span>
<span class="sd">        uniquified automatically.</span>
<span class="sd">      shape: The variable&#39;s shape.</span>
<span class="sd">      dtype: The variable&#39;s dtype.</span>
<span class="sd">      constraint: An optional projection function to be applied to the variable</span>
<span class="sd">        after being updated by an `Optimizer` (e.g. used to implement norm</span>
<span class="sd">        constraints or value constraints for layer weights). The function must</span>
<span class="sd">        take as input the unprojected Tensor representing the value of the</span>
<span class="sd">        variable and return the Tensor for the projected value</span>
<span class="sd">        (which must have the same shape). Constraints are not safe to</span>
<span class="sd">        use when doing asynchronous distributed training.</span>
<span class="sd">      synchronization: Indicates when a distributed a variable will be</span>
<span class="sd">        aggregated. Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableSynchronization`. By default the synchronization is set to</span>
<span class="sd">        `AUTO` and the current `DistributionStrategy` chooses</span>
<span class="sd">        when to synchronize.</span>
<span class="sd">      aggregation: Indicates how a distributed variable will be aggregated.</span>
<span class="sd">        Accepted values are constants defined in the class</span>
<span class="sd">        `tf.VariableAggregation`.</span>
<span class="sd">      extra_handle_data: Optional, another resource handle or Tensor with handle</span>
<span class="sd">        data to merge with `shape` and `dtype`.</span>
<span class="sd">      distribute_strategy: The tf.distribute.Strategy this variable is being</span>
<span class="sd">        created inside of.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">init_scope</span><span class="p">():</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;Variable&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">handle_name</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_from_scope_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">handle_name</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="n">shared_name</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">unique_id</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">handle_name</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">uid</span><span class="p">())</span>
          <span class="n">shared_name</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">shared_name</span><span class="p">(</span><span class="n">unique_id</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">variable_handle_from_shape_and_dtype</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shared_name</span><span class="o">=</span><span class="n">shared_name</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">graph_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">,</span>
            <span class="n">extra_handle_data</span><span class="o">=</span><span class="n">extra_handle_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
          <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s2">&quot;Read&quot;</span><span class="p">):</span>
            <span class="c1"># Manually assign reads to the handle&#39;s device to avoid log</span>
            <span class="c1"># messages.</span>
            <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">handle</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
              <span class="n">value</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
              <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">graph_element</span> <span class="o">=</span> <span class="n">value</span>
          <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
          <span class="c1"># Do *not* add to TRAINABLE_VARIABLES here, even if self._trainable,</span>
          <span class="c1"># because retraining or frozen use of imported SavedModels is</span>
          <span class="c1"># controlled at higher levels of model building.</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">UninitializedVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">distribute_strategy</span><span class="o">=</span><span class="n">distribute_strategy</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">unique_id</span><span class="o">=</span><span class="n">unique_id</span><span class="p">,</span> <span class="n">handle_name</span><span class="o">=</span><span class="n">handle_name</span><span class="p">,</span> <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
        <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span> <span class="n">graph_element</span><span class="o">=</span><span class="n">graph_element</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span> <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">)</span>


<span class="n">pywrap_tensorflow</span><span class="o">.</span><span class="n">RegisterType</span><span class="p">(</span><span class="s2">&quot;ResourceVariable&quot;</span><span class="p">,</span> <span class="n">ResourceVariable</span><span class="p">)</span>
<span class="n">math_ops</span><span class="o">.</span><span class="n">_resource_variable_type</span> <span class="o">=</span> <span class="n">ResourceVariable</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="k">def</span> <span class="nf">_dense_var_to_tensor</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">var</span><span class="o">.</span><span class="n">_dense_var_to_tensor</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">as_ref</span><span class="o">=</span><span class="n">as_ref</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>


<span class="c1"># Register a conversion function which reads the value of the variable,</span>
<span class="c1"># allowing instances of the class to be used as tensors.</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_tensor_conversion_function</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">,</span>
                                        <span class="n">_dense_var_to_tensor</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_UnreadVariable</span><span class="p">(</span><span class="n">BaseResourceVariable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents a future for a read of a variable.</span>

<span class="sd">  Pretends to be the tensor if anyone looks.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">in_graph_mode</span><span class="p">,</span> <span class="n">deleter</span><span class="p">,</span>
               <span class="n">parent_op</span><span class="p">,</span> <span class="n">unique_id</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">EagerTensor</span><span class="p">):</span>
      <span class="n">handle_name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">handle_name</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">name</span>
    <span class="c1"># Only create a graph_element if we&#39;re in session.run-land as only</span>
    <span class="c1"># session.run requires a preexisting tensor to evaluate. Otherwise we can</span>
    <span class="c1"># avoid accidentally reading the variable.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">()</span>
        <span class="ow">or</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">_building_function</span><span class="p">):</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">graph_element</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">parent_op</span><span class="p">]):</span>
        <span class="n">graph_element</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span>
            <span class="n">handle</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle</span><span class="p">,</span> <span class="n">graph_element</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">handle</span><span class="o">=</span><span class="n">handle</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">handle_name</span><span class="o">=</span><span class="n">handle_name</span><span class="p">,</span>
        <span class="n">unique_id</span><span class="o">=</span><span class="n">unique_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">handle_deleter</span><span class="o">=</span><span class="n">deleter</span><span class="p">,</span>
        <span class="n">graph_element</span><span class="o">=</span><span class="n">graph_element</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span> <span class="o">=</span> <span class="n">parent_op</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_graph_mode</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="o">.</span><span class="n">name</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="s2">&quot;UnreadVariable&quot;</span>

  <span class="k">def</span> <span class="nf">value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">read_value</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_read_variable_op</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span><span class="p">]):</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">read_variable_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">)</span>
      <span class="n">_maybe_set_handle_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handle</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">result</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The op for this variable.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parent_op</span>


<span class="n">ops</span><span class="o">.</span><span class="n">register_dense_tensor_like_type</span><span class="p">(</span><span class="n">_UnreadVariable</span><span class="p">)</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;ReadVariableOp&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_ReadGrad</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for read op.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
  <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span>
      <span class="n">handle</span><span class="p">,</span> <span class="s2">&quot;_handle_data&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">handle</span><span class="o">.</span><span class="n">_handle_data</span><span class="o">.</span><span class="n">is_set</span><span class="p">:</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>
  <span class="n">shape_proto</span> <span class="o">=</span> <span class="n">handle</span><span class="o">.</span><span class="n">_handle_data</span><span class="o">.</span><span class="n">shape_and_type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># pylint: disable=protected-access</span>
  <span class="k">if</span> <span class="n">shape_proto</span><span class="o">.</span><span class="n">unknown_rank</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">shape_proto</span><span class="o">.</span><span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gen_resource_variable_ops</span><span class="o">.</span><span class="n">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">shape_proto</span><span class="o">.</span><span class="n">dim</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">out_type</span><span class="p">)</span>


<span class="nd">@ops</span><span class="o">.</span><span class="n">RegisterGradient</span><span class="p">(</span><span class="s2">&quot;ResourceGather&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_GatherGrad</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Gradient for gather op.&quot;&quot;&quot;</span>
  <span class="c1"># Build appropriately shaped IndexedSlices</span>
  <span class="n">handle</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">params_shape</span> <span class="o">=</span> <span class="n">variable_shape</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
  <span class="n">size</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">values_shape</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">params_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">values</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">values_shape</span><span class="p">)</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">IndexedSlices</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">params_shape</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_to_proto_fn</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">export_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Converts Variable and ResourceVariable to VariableDef for collections.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">to_proto</span><span class="p">(</span><span class="n">export_scope</span><span class="o">=</span><span class="n">export_scope</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_from_proto_fn</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates Variable or ResourceVariable from VariableDef as needed.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">is_resource</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">ResourceVariable</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">variables</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_proto</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">import_scope</span><span class="o">=</span><span class="n">import_scope</span><span class="p">)</span>


<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">MOVING_AVERAGE_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">MODEL_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_STEP</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">register_proto_function</span><span class="p">(</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">METRIC_VARIABLES</span><span class="p">,</span>
    <span class="n">proto_type</span><span class="o">=</span><span class="n">variable_pb2</span><span class="o">.</span><span class="n">VariableDef</span><span class="p">,</span>
    <span class="n">to_proto</span><span class="o">=</span><span class="n">_to_proto_fn</span><span class="p">,</span>
    <span class="n">from_proto</span><span class="o">=</span><span class="n">_from_proto_fn</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_resource_variable</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;&quot;Returns True if `var` is to be considered a ResourceVariable.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">BaseResourceVariable</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span>
      <span class="n">var</span><span class="p">,</span> <span class="s2">&quot;_should_act_as_resource_variable&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">copy_to_graph_uninitialized</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Copies an existing variable to a new graph, with no initializer.&quot;&quot;&quot;</span>
  <span class="c1"># Like ResourceVariable.__deepcopy__, but does not set an initializer on the</span>
  <span class="c1"># new variable.</span>
  <span class="c1"># pylint: disable=protected-access</span>
  <span class="n">new_variable</span> <span class="o">=</span> <span class="n">UninitializedVariable</span><span class="p">(</span>
      <span class="n">trainable</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
      <span class="n">constraint</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">_constraint</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
      <span class="n">name</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">_shared_name</span><span class="p">,</span>
      <span class="n">synchronization</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">synchronization</span><span class="p">,</span>
      <span class="n">aggregation</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">aggregation</span><span class="p">,</span>
      <span class="n">extra_handle_data</span><span class="o">=</span><span class="n">var</span><span class="o">.</span><span class="n">handle</span><span class="p">)</span>
  <span class="n">new_variable</span><span class="o">.</span><span class="n">_maybe_initialize_trackable</span><span class="p">()</span>
  <span class="c1"># pylint: enable=protected-access</span>
  <span class="k">return</span> <span class="n">new_variable</span>

<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;Assert&quot;</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;VarIsInitializedOp&quot;</span><span class="p">)</span>
<span class="n">ops</span><span class="o">.</span><span class="n">NotDifferentiable</span><span class="p">(</span><span class="s2">&quot;VariableShape&quot;</span><span class="p">)</span>
</pre></div>

    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
      
    </p>
    <p>
        &copy; Copyright Copyright 2018, zfit.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.3.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>